<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assignment 4</title>
    <link rel="stylesheet" href="templates/assignment3.css">
</head>
<body>
    <h1>Exercise 4: AprilTag Detection & Safety on Robots</h1>
    
    <section class="deliverables">
        <h1>Part 1: AprilTag Detection</h1>
        
        <h2>Camera Distortion</h2>
        <p>Camera distortion can cause perspective errors in images, impacting the accuracy of AprilTag detection. To correct this, we utilize the camera's intrinsic parameters:</p>
        <ul>
            <li><strong>Subscription to Camera Topic:</strong> The robot captures images from <code>/camera_node/image/compressed</code>.</li>
            <li><strong>Undistortion Process:</strong> Using parameters from <code>/camera_node/camera_info</code>, we apply <code>cv2.undistort()</code> to correct radial and tangential distortion.</li>
        </ul>
        <p>This is exactly the same as what we did in assignment 3. This was already explained in detail in previous assignment. More details on undistortion on:     <a href="assignment3.html">Assignment 3</a> </p>
        
        <h2>Image Preprocessing</h2>
        <p>Before detecting AprilTags, we enhance image clarity using multiple preprocessing techniques:</p>
        <ul>
            <li><strong>Grayscale Conversion:</strong> Reduces computational complexity and improves edge detection.</li>
            <li><strong>Cropping:</strong> Focuses on the area of interest, removing unnecessary background.</li>
            <li><strong>Gaussian Blur:</strong> Reduces noise while maintaining sharp contours.</li>
        </ul>

        <p>
            The AprilTag detection pipeline in our Duckiebot follows these key steps:
        </p>
        <ul>
            <li><strong>Image Capture:</strong> The camera captures a continuous feed from the <code>/camera_node/image/compressed</code> topic.</li>
            <li><strong>Undistortion:</strong> The raw images are corrected using camera parameters retrieved from <code>/camera_node/camera_info</code>.</li>
            <li><strong>Preprocessing:</strong> The image is converted to grayscale, cropped, and filtered to remove noise.</li>
            <li><strong>Tag Detection:</strong> The <code>dt_apriltags</code> library scans the image for valid AprilTags, extracts their IDs, and computes their position.</li>
            <li><strong>Data Publishing:</strong> The detected tag data, including ID, position, and confidence score, is published to <code>/camera_apriltag/detections</code>.</li>
        </ul>
        
        <h2>Publishing April-tag information</h2>
        <p>
            Once an AprilTag is detected, the information is published to multiple topics to ensure other components of the Duckiebot can use the data effectively:
        </p>
        <ul>
            <li><code>/camera_apriltag/detections</code>: Contains detailed detection data such as tag ID, position, and orientation.</li>
            <li><code>/camera_apriltag/image/compressed</code>: Publishes an annotated image where detected AprilTags are marked with bounding boxes and labels.</li>
        </ul>
        

        <h2>Efficient AprilTag Detection</h2>
        <p>AprilTag detection plays a crucial role in guiding the Duckiebot in an autonomous environment. In this section, we evaluate the detection process based on efficiency, frequency, and system performance.</p>
        
        <h3>1. What is a reasonable detection rate?</h3>
        <p>
            The system operates at a detection frequency of <strong>10Hz</strong>, meaning the AprilTag detection algorithm processes <strong>10 frames per second (FPS)</strong>. 
            This rate ensures a balance between computational efficiency and real-time performance. 
        </p>
        <p>
            A lower detection rate (< 5Hz) would lead to delayed responses, causing the robot to react slowly to AprilTags. On the other hand, a higher frequency (> 15Hz) 
            would introduce unnecessary computational overhead, leading to performance issues such as increased CPU usage, delayed motor actuation, and potential frame loss in ROS.
        </p>
        
        <h3>2. Using AprilTags as Traffic Signs</h3>
        <p>
            When using AprilTags as traffic signs, continuous monitoring is essential for accurate navigation. The chosen detection rate of <strong>10Hz</strong> allows the Duckiebot to check for tags in near real-time 
            while maintaining stable performance. The logic behind this frequency is:
        </p>
        <ul>
            <li><strong>Real-time Responsiveness:</strong> 10 detections per second ensures that the Duckiebot can recognize tags within milliseconds of encountering them.</li>
            <li><strong>Motion Adaptability:</strong> As the Duckiebot moves, the AprilTags must be detected quickly enough to allow reaction before passing the tag.</li>
            <li><strong>Ensuring Stability:</strong> Running the detection at 10Hz prevents unnecessary resource consumption while still being fast enough for smooth decision-making.</li>
        </ul>
        
        <h3>3. Impact on Other Control Algorithms</h3>
        <p>
            The detection rate directly impacts other core algorithms responsible for navigation and movement control. A poorly optimized rate can lead to:
        </p>
        <ul>
            <li><strong>Delays in Obstacle Avoidance:</strong> If detection is too slow, the Duckiebot may not react in time to stop at intersections.</li>
            <li><strong>Increased CPU Load:</strong> Higher rates could slow down other processes such as motor control and lane following.</li>
            <li><strong>Frame Overlaps:</strong> A high detection rate might lead to redundant detections, causing multiple stop signals for the same tag.</li>
        </ul>
        <p>
            A detection rate of <strong>10Hz</strong> allows the Duckiebot to balance computational efficiency with responsiveness, ensuring that other ROS nodes, such as motor control and LED feedback, 
            are not overloaded.
        </p>
        
        <h3>4. Justification of Detection Rate</h3>
        <p>
            The choice of <strong>10Hz</strong> is based on empirical testing, ensuring that the Duckiebot can recognize AprilTags within an optimal time frame. The reasoning behind this decision includes:
        </p>
        <ul>
            <li><strong>Human Reaction Benchmark:</strong> The average human reaction time is ~250ms. At 10Hz, the Duckiebot checks its environment every 100ms, ensuring a response time faster than a human driver.</li>
            <li><strong>AprilTag Visibility Window:</strong> Based on testing, an AprilTag remains visible in the camera frame for approximately <strong>0.5 - 1.2 seconds</strong>. With a 10Hz detection rate, at least <strong>5-12 frames</strong> can be processed per tag appearance, reducing the risk of missing detections.</li>
            <li><strong>Computational Constraints:</strong> Running at 10Hz ensures that the Duckiebot does not exceed its processing capacity while maintaining a steady ROS loop rate.</li>
        </ul>
        

        <h3>AprilTag Detection Demonstration</h3>
        <video width="auto" height="600" controls>
            <source src="videos/april_tag.webm" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        <h3>Detecting april tag and moving</h3>
        <video width="auto" height="600" controls>
            <source src="videos/output.webm" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        <p>This video was captured ont the <code>/camera_apriltag/image/compressed</code> topic while running the tag detection and changing led and moving code</p>
    
    

        <h2>LED Changes Based on AprilTag Detection</h2>
        <p>Upon detecting an AprilTag, the Duckiebot immediately updates its LED color to signal different road conditions. This ensures clear, real-time visual feedback for navigation.</p>
        
        <ul>
            <li><strong>Red LED:</strong> Stop Sign detected.</li>
            <li><strong>Blue LED:</strong> T-Intersection detected.</li>
            <li><strong>Green LED:</strong> UofA Tag detected.</li>
            <li><strong>White LED:</strong> No detection (default state).</li>
        </ul>
        
        <h3>How the Code Works</h3>
        <p>
            We subscribe to the topic we created before <code>/camera_apriltag/detections</code> where detected tag information is received. The program extracts the tag ID and, based on predefined categories where category have their own tagID
            (Stop Sign, T-Intersection, UofA Tag), assigns the corresponding LED color. The LED update is executed immediately upon detection to ensure real-time feedback.
        </p>
        
        <h2>Stopping Based on AprilTag Detection</h2>
        <p>The Duckiebot stops for a specific duration based on the last detected AprilTag:</p>
        <ul>
            <li><strong>3 seconds:</strong> Stop Sign detected.</li>
            <li><strong>2 seconds:</strong> T-Intersection detected.</li>
            <li><strong>1 second:</strong> UofA Tag detected.</li>
            <li><strong>0.5 seconds:</strong> No detection (default state).</li>
        </ul>
        <ul>
            <li>
                <h3>A video of the Duckiebot detecting stop tag changing the lights to red and stopping for 3 second</h3>
                <video width="auto" height="600" controls>
                    <source src="videos/stop.mp4" type="video/mp4">
                    Your 
                </video>
            </li>
            <li>
                <h3>A video of the Duckiebot detecting T-intersection tag then changing the lights to blue and stopping for 2 second</h3>
                <video width="auto" height="600" controls>
                    <source src="videos/t-inter.mp4" type="video/mp4">
                    Your 
                </video>
            </li>
            <li>
                <h3>A video of the Duckiebot detecting U-ofA tag them changing the lights to blue and stopping for 1 second</h3>
                <video width="auto" height="600" controls>
                    <source src="videos/ualberta.mp4" type="video/mp4">
                    Your 
                </video>
            </li>
            <li>
                <h3>A video of the Duckiebot detecting no tag and stopping for 0.5 second</h3>
                <video width="auto" height="600" controls>
                    <source src="videos/non.mov" type="video/mp4">
                    Your 
                </video>
            </li>
        </ul>
        <p>
            In the beginning, we detect the red line using the HSV values similar to the ones we used in assignment 3 also detect how far it is using similar logic as in assignment 3.
            Then we see if there are april-tags.
            After detecting an AprilTag, the program assigns a predefined stop duration. Then we change the led to the according to the detected tag. We then call the move_straight function with the distance we calculated the red line to be at.  The Duckiebot halts for the required time, according rules when it reaches that distance. Once the waiting period ends,
            it moves forward by 30 cm.
        </p>
        
        <h3>Impact of Detection Rate on Intersection Performance</h3>
        <p>
            The AprilTag detection rate significantly affects intersection behavior and stopping performance. 
            At <strong>10Hz</strong>, the system can process detections in near real-time, ensuring that the Duckiebot reacts promptly to intersection signs. 
        </p>
        <ul>
            <li><strong>Lower detection rate (&lt; 5Hz):</strong> Could cause the Duckiebot to miss detections and fail to stop at intersections.</li>
            <li><strong>Higher detection rate (&gt; 15Hz):</strong> Increases CPU load and could introduce redundant stop signals for the same tag.</li>
        </ul>
        <p>Thus, a <strong>10Hz detection rate</strong> ensures a balance between efficiency and responsiveness.</p>
    </section>
    
    <section class="deliverables">

        <h1>Part Two - PeDuckstrian Crosswalks</h1>
        <h2>1. Detecting Crosswalks with Double Blue Lines</h2>
        <p>
        In this section, the Duckiebot is required to detect crosswalks marked by two blue lines and stop at the <strong>first blue line</strong>, regardless of whether peDuckstrians are crossing. We implemented this using an HSV-based color segmentation pipeline enhanced with contrast boosting using CLAHE (Contrast Limited Adaptive Histogram Equalization). First, the image is undistorted using camera parameters from <code>/camera_info</code> and enhanced using:
        </p>
        <pre>clahe = cv2.createCLAHE(...)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)</pre>
        
        <p>
        During testing, we observed that the blue tape lines were hard to distinguish from the black road at a distance due to poor contrast and similar darkness. To address this, we used contrast enhancement with CLAHE to improve visibility of the blue features even in low light or dark surfaces.
        </p>
        
        <ul>
            <li><strong>HSV Thresholding:</strong> We apply a binary mask using the range <code>[85, 50, 50] to [130, 255, 255]</code> to isolate blue features.</li>
            <li><strong>Contour Filtering:</strong> We then filter for wide, horizontal rectangles by checking that the aspect ratio is > 3 and area > 100:
                <pre>aspect_ratio = float(w) / h</pre>
            </li>
            <li><strong>Double Line Handling:</strong> The crosswalk has two lines, so we sort the valid contours by vertical position and select the lowest one using:
                <pre><code>valid.sort(key=lambda c: cv2.boundingRect(c)[1], reverse=True)</code></pre>
                We then estimate its distance with pinhole projection:
                <pre><code>distance = (real_width * focal_px) / pixel_width</code></pre>
            </li>
        </ul>
        <p>
        Once distance is estimated, the Duckiebot stops using <code>navigator.move_straight()</code> followed by <code>navigator.stop(1)</code>. It ignores the second line to prevent double stops.
        </p>
        
        <h2>2. PeDuckstrian Detection</h2>
        <p>
        After stopping, we use the lower 43% of the image to check for yellow ducks with red beaks:
        </p>
        <pre><code>roi = image[int(0.43 * height):]</code></pre>
        
        <p>
        Initially, we encountered false positives because the detector was identifying distant yellow objects high in the frame, far from the crosswalk. These detections were not meaningful, as they weren't in the robot's path. To resolve this, we cropped the input image to only look at the lower portion where peDuckstrians are realistically located.
        </p>
        
        <ul>
            <li><strong>Yellow Detection:</strong> Applied via HSV mask <code>[15, 80, 120] to [35, 255, 255]</code></li>
            <li><strong>Red Detection:</strong> Combined two red ranges around 0° and 180°:
                <pre><code>red_mask = cv2.bitwise_or(mask1, mask2)</code></pre>
            </li>
            <li><strong>Validation:</strong> Yellow areas must have at least 10 red pixels in the same bounding box to be counted as a duck.</li>
            <li><strong>Stabilization:</strong> We use a buffer of N frames, and only trigger a state change if a majority confirm:
                <pre><code>recent_detections.pop(0); recent_detections.append(detected)</code></pre>
            </li>
        </ul>
        
        <h2>3. PeDuckstrian Handling Logic</h2>
        <p>
        If ducks are present, the Duckiebot waits until 4 consecutive frames confirm clearance:
        </p>
        <pre><code>if not detected: consecutive_clear += 1</code></pre>
        <p>
        LED is red while waiting, then green when clear. 
        </p>
        
        <h3>Video of bot moving straight if bot is detected and waiting to clear if duckiebot are present</h3>
        <video width="auto" height="600" controls>
            <source src="videos/duckies.mov" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        <h2>ROS Communication</h2>
        <ul>
            <li>Subscribed to camera images from <code>/&lt;VEHICLE_NAME&gt;/camera_node/image/compressed</code>.</li>
            <li>Subscribed to camera info from <code>/&lt;VEHICLE_NAME&gt;/camera_node/camera_info</code> for undistortion and focal length.</li>
            <li>Published estimated distance to the first crosswalk line to <code>/&lt;VEHICLE_NAME&gt;/crosswalk_distance</code> as a <code>Float32</code>.</li>
            <li>Published peDuckstrian detection state (clear or blocked) to <code>/&lt;VEHICLE_NAME&gt;/peduckstrian_detection/clear</code> as a <code>Bool</code>.</li>
            <li>Optionally published debug images (duck detection overlays) to <code>/&lt;VEHICLE_NAME&gt;/camera_ducks/image/compressed</code>.</li>
        </ul>
        <h2>Execution Flow</h2>
        <p>
        During execution, the Duckiebot follows this sequence to safely handle crosswalks and peDuckstrians:
        </p>
        <ol>
            <li><strong>Crosswalk Detection:</strong> The robot continuously scans for blue lines using HSV filtering with contrast-enhanced input. If two horizontal blue lines are detected, the bot targets the first one (closest).</li>
            <li><strong>Distance Estimation:</strong> Once the first line is found, the system estimates its distance using the width of the line in pixels and the camera's focal length.</li>
            <li><strong>Approach First Line:</strong> The Duckiebot drives forward to just before the first blue line (typically 3–5 cm behind it) using <code>navigator.move_straight()</code>.</li>
            <li><strong>Stop and Wait:</strong> Upon arrival at the line, it stops and pauses for 1 second regardless of peDuckstrian presence.</li>
            <li><strong>PeDuckstrian Detection:</strong> After stopping, the Duckiebot scans the lower portion of the image for yellow + red color patterns that match the peDuckstrian ducks.</li>
            <li><strong>Wait if Duck is Present:</strong> If a peDuckstrian is detected, the bot waits at the line until 4 consecutive frames confirm the area is clear (using a rolling buffer of detections).</li>
            <li><strong>Proceed After Clearance:</strong> Once no duck is detected, the bot turns the LED to green and moves forward past the second line (~30–40 cm) to clear the crosswalk.</li>
        </ol>
        <p>
        This ensures the Duckiebot always stops at crosswalks, waits for pedestrians, and only resumes when it's safe to proceed.
        </p>

        

        
        <h2>Summary for part 2</h2>
        <ul>
            <li>Only stops at the first crosswalk line using <code>cv2.boundingRect</code> sorting logic.</li>
            <li>Duck detection requires yellow+red combo to confirm, preventing false alarms.</li>
            <li>Cropped ROI ensures that only nearby ducks trigger stop behavior.</li>
            <li>A buffer of recent detections smooths out noise, using simple list-based logic.</li>
            <li>CLAHE contrast enhancement solves visibility issues due to dark road and tape colors.</li>
        </ul>
    </section>
    <section class="deliverables">

        <h1>Part Three - Safe Navigation</h1>

        <h2>1. Detecting Another Duckiebot</h2>
        <p>
        To detect a broken-down Duckiebot ahead, we leveraged its distinct pattern of black circular dots on a white background. Since we always approach the other bot from behind, this pattern is clearly visible from ~30 cm away. Our detection pipeline uses contrast-enhanced grayscale images followed by blob filtering using OpenCV’s <code>SimpleBlobDetector</code>.
        </p>

        <p>
            We observed that the back of the Duckiebot has a distinct pattern of black circular dots arranged in a grid-like fashion. Based on this, our first idea was to use that visual signature as the detection cue. We built the detection pipeline specifically around this pattern, using blob detection to isolate circular features and estimate distance based on their spread. This approach worked much better than color or shape-based methods in our tests.
            </p>
            
            <pre># cv2.SimpleBlobDetector_Params used
                params.filterByArea = True       
                params.minArea = 3      
                params.maxArea = 80         
                params.filterByCircularity = True       
                params.minCircularity = 0.85</pre>
                

        <p>
        To reduce false positives from noise or distant black features, we applied several preprocessing steps:
        </p>
        <ul>
            <li><strong>Cropping:</strong> Focus on the center of the image by removing top/bottom 30% and side 40%.</li>
            <li><strong>CLAHE:</strong> Improve contrast in varying lighting conditions.</li>
            <li><strong>Adaptive Thresholding:</strong> Better binary segmentation under non-uniform light.</li>
            <li><strong>Contour Filtering:</strong> Final mask keeps only circular blobs using circularity calculation.</li>
        </ul>

        <p>
        We faced a challenge where distant dots appeared too small and were filtered out. Also, in some frames, random noise would trigger false dots. To handle this, we:
        </p>
        <ul>
            <li>Used a <strong>5-frame smoothing buffer</strong> to stabilize distance estimates:</li>
            <pre>self.history.append(distance)
            return np.mean(self.history)</pre>
            <li>Published detection state (clear or blocked) on <code>/duckiebot_detection/clear</code> as a <code>Bool</code>.</li>
            <li>Published estimated distance (in cm) to <code>/duckiebot_detection/distance</code> as a <code>Float32</code>.</li>
            <li>Optionally published debug images to <code>/camera_ducks/image/compressed</code>.</li>
        </ul>

        <h2>2. Pause to Assess</h2>
        <p>
        Once the pattern was detected, we used the distance estimate to stop approximately <strong>15 cm behind</strong> the detected Duckiebot. If the robot was already within 15 cm, we skipped forward motion and paused:
        </p>
        <p>
            In the <code>auto_nav_func() </code> We have following methods:
            <pre>
                sharp_left() : Turns sharp 90 degeree left
                move_straight(DISTANCE_IN_M): Moves the straight the distance provided
                sharp_right() : Turns sharp right 90 degree
            </pre>
        <pre>if self.distance > 0.15:
            self.navigator.move_straight(self.distance - 0.18)</pre>
        <p>
        We stopped for 3 seconds to simulate damage assessment before attempting a maneuver. LED behavior was also controlled to indicate the stop state.</p>

        <h2>3. Maneuver Around the Broken Duckiebot</h2>
        <p>
        After the pause, we executed a planned avoidance maneuver:
        </p>
        <ol>
            <li><strong>Turn left 90°</strong> to shift into the opposing lane.</li>
            <li><strong>Drive forward</strong> 8–10 cm to align.</li>
            <li><strong>Turn right 90°</strong> to face forward.</li>
            <li><strong>Drive straight</strong> for ~45 cm to bypass the bot.</li>
            <li><strong>Turn right</strong> again, move forward slightly, and finally <strong>turn left</strong> back into the correct lane.</li>
        </ol>


        All turns were executed using the navigator methods like <code>navigator.sharp_left()</code> and <code>navigator.sharp_right()</code>. This path ensured the Duckiebot made no physical contact with the broken one and respected lane boundaries.
        </p>

        <h2>4. Continue Driving as Usual</h2>
        <p>
        After completing the lane switch and passing the obstacle, we transitioned back into the original lane and resumed normal driving. The Duckiebot drove an additional ~30 cm forward to complete the sequence:
        </p>
        <pre>self.navigator.move_straight(0.3)</pre>
        <h3>Video of bot moving straight if bot is detected and waiting to clear if duckiebot are present</h3>
        <video width="auto" height="600" controls>
            <source src="videos/bot_detect.mov" type="video/mp4">
            Your browser does not support the video tag.
        </video>

        <h2>Challenges</h2>
        <ul>
            <li>Distant blobs were often filtered out or merged due to their small size.</li>
            <li>Some frames contained noise interpreted as dots; fixed using binary mask cleanup and detection history buffer.</li>
            <li>Stable distance estimation used smoothed rolling average of last 5 valid frames.</li>
        </ul>
                
    </section>
    
    <section class="references">
        <h2>References</h2>
        <ul>
            <li> 
                <a href="https://learnopencv.com/blob-detection-using-opencv-python-c/">Open CV Blob detection</a>
            </li>
            <li>
                <a href="https://www.geeksforgeeks.org/clahe-histogram-eqalization-opencv/">geeksforgeeks CLAHE</a>
            </li>
            <li>ChatGPT was used with report and some parts of coding</li>
                </ul>
    </section>
    
    <h3>By: Sandhya Adhikari and Fumanpreet Singh</h3>
    <a href="index.html">Back to Assignments</a>
</body>
</html>
