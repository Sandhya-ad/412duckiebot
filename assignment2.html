<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assignment 2 - ROS Development and Kinematics</title>
    <link rel="stylesheet" href="assignment2.css">
</head>
<body>
    <h1>Assignment 2 - ROS Development and Kinematics</h1>

    
    <section class="description">
        <p>
            This assignment focuses on understanding the <strong>Robot Operating System (ROS)</strong> and its core principles in <strong>robot kinematics</strong> and <strong>odometry</strong>. 
            We implemented ROS nodes, created a publisher and subscriber, wrote code for the duckie to move straight, reverse, rotate and drive in D.
        </p>
    </section>

    <section class="deliverables">
        <h1>Deliverables</h1>
        <ul>
            <li>
                <h2>ROS Wiki Documentation</h2>
                <div class="report">
                    <h3>1. What is a ROS Node, and what is its role in the ROS ecosystem?</h3>
                    <p>
                        A <strong>node</strong> in ROS is an executable within a ROS package that performs a specific computational task. It can be thought of as an independent module in a robotic system, 
                        responsible for a specific task such as sensor data processing, motor control, or decision-making.
                    </p>
                    <p>
                        ROS nodes enable a <strong>modular architecture</strong> where different functionalities are distributed across multiple independent processes. These nodes communicate with each
                        other using topics (publish/subscribe) or services (request/response). This structure enhances <strong>organization, scalability, and fault tolerance</strong>, ensuring that a 
                        failure in one node does not disrupt the entire system. 
                        Example: A camera node might be responsible for capturing images, while a processing node analyzes those images.
                    </p>

                    <h3>2. What are ROS topics, and how do they facilitate communication between nodes?</h3>
                    <p>
                        <strong>ROS Topics</strong> act as a communication channel that allows nodes to exchange messages asynchronously. They follow a publish-subscribe model, meaning Publishers send 
                        data to a topic and Subscribers receive data from a topic.
                    </p>
                    <p>
                        A <strong>Publisher</strong> node generates and sends data to a specific topic, while any <strong>Subscriber</strong> node listening to that topic retrieves the information.
                        Topics are loosely coupled, meaning a publisher does not need to know about subscribers and vice versa. This ensures <strong>scalability</strong> and <strong>efficiency</strong>.
                        Example: A camera node publishes images to the topic /camera/image_raw, while an object detection node subscribes to this topic to process the images.
                    </p>

                    <h3>3. What are ROS services, and how do they differ from topics?</h3>
                    <p>
                        <strong>ROS Services</strong>  are another way that nodes can communicate with each other. They provide a <strong>request-response</strong> type of communication between the nodes. 
                        Unlike topics, where a publisher continuously streams messages and subscribers listen (e.g., sensor data), whereas services operate on a request-response model where a node sends
                        a request to a service, and the service responds with the necessary information or action (e.g., requesting robot status). This synchronous interaction makes services ideal for 
                        tasks that require immediate execution and direct response.
                    </p>

                    <h3>4. What are ROS messages, and how are they structured for communication?</h3>
                    <p>
                        <strong>ROS messages</strong> define the structure of data exchanged between nodes through topics, services, and actions. They ensure different nodes can correctly interpret
                        and process the data being transmitted.
                    </p>
                    <p>
                        Example: In the case of the <strong>Duckiebot's camera sensor</strong>, the topic used to publish camera frames is: <br><code>/ROBOT_NAME/camera_node/image/compressed</code>
                    </p>
                    <p>
                        The message type used over this topic is the standard <code>sensor_msgs/CompressedImage</code>, which contains the following fields:
                    </p>
                    <ul>
                        <li><strong>std_msgs/Header header</strong>: Contains a timestamp and frame reference.</li>
                        <li><strong>string format</strong>: Specifies the format of the data, such as <code>png</code> or <code>jpeg</code>.</li>
                        <li><strong>uint8[] data</strong>: An array of bytes containing the actual image in the specified format.</li>
                    </ul>
                    <p>
                        This structured message format allows the Duckiebot to efficiently publish and process camera images, enabling vision-based tasks such as lane detection and object tracking.
                    </p>
                    

                    <h3>5. What is a ROS bag, and how is it used in data recording and playback?</h3>
                    <p>
                        A <strong>ROS bag</strong> is a file format for recording and playback ROS topics. This feature is useful for <strong>debugging, analysis, and simulation</strong> of
                        robotic behavior without needing real hardware.  
                    </p>
                    <p>
                        Writing a rosbag file:
                        <ul>
                            <li>Open a terminal and execute: <code>dts start_gui_tools</code></li>
                            <li>Run: <code>rosbag record topic -O move.bag</code>, where <code>topic</code> is your wheel command topic.</li>
                            <li>Open a separate terminal and execute your node using: <code>dts devel run -R csc229XX -L ....</code></li>
                            <li>Once it’s finished, go back to your first terminal and press <code>Ctrl + C</code> to stop recording the bag file.</li>
                            <li>Now, you have your ROSBag file inside your Docker container. You need to move it locally to your PC.</li>
                        </ul>

                    </p>

                    <h3>6. Typical workflow for setting up communication between multiple nodes in ROS.</h3>
                    <p>
                        Assuming that you have Duckiebots which uses ROS1, and ROS is already installed on them through the Docker which was covered in Exercise 1. Now, create a new ROS-compatible DTProject
                        - an interface for using ROS with duckiebots which is available at <a href="https://github.com/duckietown/template-ros/">duckietown/template-ros</a>. 
                    </p>
                    <p>
                         Now, create a new catkin package inside a DTProject because ROS uses the catkin build system to organize and build its software. In a nutshell, catkin organizes entire projects in the so-called catkin workspaces. A catkin workspace is nothing more than a directory containing a bunch of software modules called 
                        catkin packages. Each software module can contain a set of executables (e.g., binaries, script files) called ROS nodes. ROS nodes interact with one another using two of the most
                        common communication patterns, called publish-subscribe and request-reply.

                        How and where to create a new Catkin package is available at <a href="https://docs.duckietown.com/daffy/devmanual-software/beginner/ros/catkin-packages.html#create-a-new-catkin-package">catkin package creation</a>.  
                    </p>
                    <p>
                        ROS implements the publish-subscribe pattern using ROS Publishers and ROS Subscribers. The general concept is simple, a publisher has the job of publishing messages from a ROS node into the ROS network for other nodes to receive (using ROS Subscribers).
                        How and where to create Publisher ROS Node is available at <a href="https://docs.duckietown.com/daffy/devmanual-software/beginner/ros/publisher.html">publisher node creation</a>. Similarly, for Subscribers, the general concept is simple: a subscriber has the job of listening for messages about a specific topic that
                        are published by other ROS nodes (using ROS Publishers) over a ROS network. How and where to create Subscriber ROS Node is available at <a href="https://docs.duckietown.com/daffy/devmanual-software/beginner/ros/subscriber.html">subscirber node creation</a>
                        <br>
                        We now need to the tell our file system that we want our file my_publisher_node.py (any publisher node) and my_subsciber_node.py (any subscriber node) be treated as an executable file. We do so by running the following command from the root of our DTProject: <code>chmod +x ./packages/my_package/src/my_publisher_node.py</code>
                        and <code>chmod +x ./packages/my_package/src/my_subscriber_node.py</code>
                    </p>
                    <p>
                        Everything in Duckietown runs inside Docker containers. This means that we also need to tell Docker what to run when the container is started. In this case, we want our new ROS publisher node to run.
                        Each DTProject compiles into a single Docker image, but we can declare multiple start “behaviors” for the same project/image so that the same project can serve multiple (though related) purposes. We can use launchers to accomplish this, now we have create a new launcher to allow for this new start behavior.
                        <br>
                        To Launch the Publisher node or Subscriber node, run the following command on terminal: 
                        <br><code>dts devel build -H ROBOT_NAME -f</code>
                        <br><code>dts devel run -H ROBOT_NAME -L my-publisher</code>
                        
                    </p>
                    
                </div>
            </li>

            <li>
                <h2>ROS Implementation Tasks</h2></li>
            <li>
                <h3>A screenshot of subscriber info from “Hello from…”</h3>
                <div class="image-container">
                    <img src="images/publisher.png" alt="ROS Subscriber" class="deliverable-photo">
                </div>
                <p>
                    The <strong>Publisher node</strong> is responsible for sending messages to a specific topic in ROS. It continuously broadcasts a text message to 
                    inform other nodes about an event, status, or relevant data. In this case, it sends a simple message with the Duckiebot's name. 
                </p>
                <p>
                    The purpose of the Publisher node is to act as a <strong>producer</strong>  ensuring that other nodes in the system can receive and react to its messages. 
                    This method of communication is useful for continuously updating information, such as sensor readings, status updates, or instructions in a 
                    robotic system.
                </p>
                
                <p><strong>Subscriber Node (MySubscriberNode)</strong></p>
                <div class="image-container">
                    <img src="images/subscriber.png" alt="ROS Subscriber" class="deliverable-photo">
                </div>
                <p>
                    The <strong>Subscriber node</strong> listens for messages being published on a specific topic. When a new message arrives, the Subscriber node 
                    retrieves and processes it. In this case, the Subscriber reads and logs the message sent by the Publisher.
                </p>
                <p>
                    The purpose of the Subscriber node is to act as a <strong>data_consumer</strong> , ensuring that the information published by other nodes is received and 
                    interpreted correctly. This model allows for an <strong>efficient and modular</strong> robotic system, where different components communicate asynchronously 
                    without direct dependencies.
                </p>
                
                <ul>
                    <li>
                        Implementing the <strong>Publisher-Subscriber model</strong> in ROS provided valuable insight into how different nodes in a robotic system 
                        communicate asynchronously.</strong>.
                    </li>
                    <li>
                        This approach allowed us to establish <strong>real-time communication</strong> between nodes, ensuring seamless data flow without dependencies. 
                        We learned how to design independent nodes that can send and receive messages autonomously, improving overall system reliability.
                    </li>
                </ul>
                
                
            <li>
                <h3>A screenshot of your robot’s camera image view in your annotated and customized
                    camera image”</h3>
                <div class="image-container">
                    <img src="images/grey.png" alt="Grey image" class="deliverable-photo">
                </div>
                <div class="report">
                    <p><strong>Explanation:</strong></p>
                
                    <p>
                        Our Duckiebot subscribes to the topic <code>/ROBOT_NAME/camera_node/image/compressed</code>. 
                        This topic continuously receives <strong>compressed image frames</strong> from the Duckiebot's <strong>front-facing camera</strong> in real-time.
                    </p>
                
                    <p>
                        When a new image is received, we use <strong>OpenCV</strong> and <strong>cv_bridge</strong> to decode the compressed image from ROS format 
                        (<code>sensor_msgs/CompressedImage</code>) into an OpenCV-compatible image. This allows us to manipulate and process the image efficiently.
                    </p>
                
                    <p>
                        After decoding, the <strong>color image</strong> is converted into a <strong>grayscale image</strong> using OpenCV’s <code>cv2.cvtColor</code> function. 
                        This removes all color information, leaving only intensity values ranging from <strong>black (0) to white (255)</strong>. Grayscale processing is often used in 
                        <strong>computer vision applications</strong> as it reduces computational complexity while preserving important details like edges and object shapes.
                    </p>
                
                    <p>
                        We then add the text to the processed image using OpenCV’s <code>cv2.putText</code> function, which places the text near the <strong>bottom-left corner</strong> of the image in <strong>green</strong> color.
                    </p>
                
                    <p>
                        Finally, the <strong>processed grayscale image</strong> (with the text annotation) is <strong>republished</strong> to the topic 
                        <code>/ROBOT_NAME/camera_processed/image/compressed</code>, allowing other ROS nodes to <strong>subscribe</strong> and view the modified image instead of the raw camera feed.
                    </p>
                
                    <p>
                        This process demonstrates real-time <strong>image acquisition, transformation, and annotation</strong> using ROS and OpenCV. It provides a foundation for further 
                        applications such as <strong>lane detection, feature tracking, and object recognition</strong> in autonomous robotics.
                    </p>
                </div>
                
            </li>

            <li>
                <h3>A video of the Duckiebot moving forward and backward for 1.25 meters</h3>
                <video width="auto" height="600" controls>
                    <source src="videos/straight_code.mov" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <div class="report">
                    <p>
                        The Duckiebot successfully moved forward and backward for <strong>1.25 meters</strong>. Initially, we set equal throttle values for both wheels, 
                        but the robot consistently veered to the left. To correct this, we adjusted the speed of each wheel individually to ensure a straight trajectory.
                    </p>
                    <p>
                        To accurately measure the distance traveled, we used <strong>encoder ticks</strong>. Given that one full wheel rotation corresponds to 
                        <strong>135 ticks</strong>, we performed the necessary calculations to determine the exact number of ticks required to cover <strong>1.25 meters</strong>. 
                        This approach gave us better motion control.
                    </p>
                </div>
                
            </li>
            <li>
                <h3>A video of the Duckiebot rotating clockwise and counter clockwise</h3>
                <video width="auto" height="600" controls>
                    <source src="videos/rotate.mov" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <div class="report">
                    <p>
                        Need to explain
                </div>
                
            </li>

            <div class="report">
                <h2>Straight-Line Motion and Rotation Task Explanation</h2>
            
                <h3>1. Straight-Line Motion</h3>
                <p>
                    The Duckiebot was programmed to move forward for <strong>1.25 meters (125 cm)</strong> and then reverse back the same distance. 
                    While the movement was mostly accurate, some deviations were observed between the <strong>actual and desired location</strong> 
                    due to minor variations in wheel speed and latency in stopping.
                </p>
                
                <h4>Mathematical Calculations for 125 cm Distance</h4>
                <p>
                    To ensure the Duckiebot traveled exactly <strong>125 cm</strong>, we calculated the required number of <strong>encoder ticks</strong> 
                    based on our measured values.
                </p>
                
                <ul>
                    <li>
                        The measured <strong>wheel circumference</strong> is <strong>21 cm</strong>.
                    </li>
                    <li>
                        From our measurements, we found that <strong>one full wheel rotation (21 cm traveled) corresponds to 135 encoder ticks</strong>.
                    </li>
                    <li>
                        To determine how many <strong>ticks correspond to 1 cm</strong>, we calculated:
                        <br>
                        <code>Ticks per cm = 135 / 21 ≈ 6.43 ticks/cm</code>
                    </li>
                    <li>
                        To find the number of <strong>ticks needed to travel 125 cm</strong>, we used:
                        <br>
                        <code>Required Ticks = (135 / 21) * 125 ≈ 805 ticks</code>
                    </li>
                </ul>
                
                <p>
                    Thus, the Duckiebot needed to move approximately <strong>805 ticks forward</strong> and then <strong>805 ticks in reverse</strong>.
                    To account for the motor command delay, we have to reduce the few ticks and set it to 750
                </p>
                <p>
                    Initially, we set equal throttle values for both wheels, but the robot veered slightly to the left. To correct this, we adjusted the 
                    <strong>left wheel throttle to 0.545</strong> and the <strong>right wheel throttle to 0.5</strong>, ensuring a straighter trajectory.
                </p>
                
            
                <h4>Why is there a difference between the actual and desired location?</h4>
                <p>
                    The Duckiebot sometimes moved slightly more . One possible reason is <strong>latency in motor commands</strong>, where the robot takes extra time to stop even after the command is executed.
                </p>
            
                <h4>What speed did you use?</h4>
                <p>
                    The speed was initially set to a 0.4 for the throttle to maintain stability. We later adjusted to around 0.5 for the right throtle and 0.545 for the left throttle and that minimized our bot's issue of going lest but it did not fix it entirely.
                </p>
            
                <h4>What happens when you increase or decrease the speed?</h4>
                <p>
                    <li> <strong>Increasing the speed</strong> made the Duckiebot move faster, but it also increased the stopping delay, causing the bot to take longer to stop or moving more than expected</li>
                    <li> <strong>Decreasing the speed</strong> would make our bot drift on one direction and also if the speed is not enough, bot would not be able to complete the rotation task</li>
                </p>
            
                <h3>2. Rotation Task</h3>
                <p>
                    The Duckiebot was programmed to rotate <strong>90 degrees clockwise (π/2 radians)</strong> and then rotate back to 
                    <strong>0 degrees counterclockwise</strong>. The movement was based on **encoder tick counts**, ensuring precise rotation.
                </p>

                    <h4>Mathematical Calculations for 90-Degree Rotation</h4>
                    <p>
                        To rotate the Duckiebot by <strong>90 degrees</strong>, we needed to determine the number of <strong><ticks/strong> required. 
                    </p>

                    <ul>
                        <li>
                            From previous measurements, we determined that <strong>one full wheel rotation corresponds to 135 encoder ticks</strong>.
                        </li>
                        <li>
                            We found that rotating 90 degrees required the <strong>left wheel to move forward for 35 ticks</strong> 
                            while the right wheel remained stationary.
                        </li>
                        <li>
                            The formula used for determining tick count:
                            <br>
                            <code>Required Ticks for 90° Rotation = Experimentally found ≈ 35 ticks</code>
                        </li>
                        <li>
                            To rotate back counterclockwise to <strong>0 degrees</strong>, the l<strong> wheel moved in reverse for the same 35 ticks</strong>
                        </li>
                    </ul>

                    <p>
                        Thus, the Duckiebot performed an accurate <strong>90-degree clockwise rotation</strong> and then returned to its original position 
                        by rotating <strong>90 degrees counterclockwise</strong>, both using <strong>35 encoder ticks per movement</strong>.
                    </p>

                    <p>
                        Initially, small deviations were observed where the bot sometimes <strong>rotated slightly more than 90 degrees</strong>. 
                        To correct this, adjustments were made by fine-tuning the motor power and ensuring the **right wheel remained completely still** 
                        during the rotation.
                    </p>

                            
                <h4>Did you observe any deviations in the rotation?</h4>
                <p>
                   Yes, It would occasionally rotated <strong>more than 90 degrees</strong> before stopping.
                </p>
            
                <h4>If deviations exist, what could be the cause?</h4>
                <p>
                    <li> <strong>Latency in motor commands:</strong> A slight delay in executing stop commands could result in extra movement.    
                    </li>
                   <li><strong>Wheel friction and drift:</strong> One wheel may have moved faster than the other, causing the Duckiebot to drift slightly.</li>
                    <li><strong>Uneven surface condition:</strong> Even small inconsistencies in the floor may have contributed to the deviations.</li>  
                </p>
            
                <p>
                    To correct the issue, adjustments were made by slightly reducing speed and fine-tuning the motor power to balance both wheels. 
                    After experimenting with different values, the correct parameters were found, and the Duckiebot successfully completed the full rotation and stopped as expected.
                </p>
            </div>
            

            <li>
                <h3>A video of the Duckiebot driving in a “D” shape</h3>
                <video width="auto" height="600" controls>
                    <source src="videos/d_shape.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <div class="report">
                    <p>The Duckiebot followed a <strong>“D” shaped trajectory</strong> using wheel encoder odometry.</p>
                    
                    <h4>LED State Indicators</h4>
                    <ul>
                        <li><strong>🔴 Red LED:</strong> Indicates that the Duckiebot is <strong>stopping for 5 seconds</strong>.</li>
                        <li><strong>🔵 Blue LED:</strong> Indicates that the Duckiebot is <strong>moving along the D-shaped path</strong>.</li>
                    </ul>
            
                    <p>
                        The LED color transitions help visualize the different motion states of the Duckiebot, ensuring clear feedback 
                        on when it is actively moving versus when it is in a stationary state.
                    </p>
                </div>
            </li>
            
            <div class="report">
                <h2>Overall Implementation Summary</h2>
            
                <h3>What We Implemented</h3>
                <p>
                    In this lab, we implemented three key components of <strong>ROS-based robotic communication and control</strong>:
                </p>
                <ul>
                    <li>A <strong>Publisher-Subscriber model</strong>, where the publisher continuously sent messages, and the subscriber listened and logged the received data.</li>
                    <li>A <strong>Camera Processing Node</strong>, which converted live camera feed images to grayscale, added annotations, and republished them for visualization.</li>
                    <li>A <strong>Motion Control Task</strong>, where the Duckiebot moved forward and backward for a set distance and performed a <strong>90-degree rotation</strong>.</li>
                </ul>
            
                <h3>How It Works</h3>
                <p>
                    The <strong>Publisher-Subscriber model</strong> demonstrated <strong>asynchronous communication</strong>, allowing different nodes to send and receive messages independently. 
                    The <strong>camera node</strong> processed images in real-time using <strong>OpenCV</strong>, while the <strong>motion task</strong> involved executing precise motor commands to control the Duckiebot’s movement.
                </p>
            
                <h3>What We Learned</h3>
                <ul>
                    <li>How <strong>ROS topics</strong> facilitate real-time message exchange between independent nodes.</li>
                    <li>The importance of <strong>asynchronous messaging</strong> for scalable robotic systems.</li>
                    <li>How <strong>image processing</strong> in ROS works using OpenCV and how images can be modified and republished.</li>
                    <li>The challenges of <strong>accurate movement control</strong> and how to fine-tune parameters for better results.</li>
                </ul>
            
                <h3>Challenges and How We Overcame Them</h3>
                <ul>
                    <li><strong>Motion Deviations:</strong> The Duckiebot sometimes moved slightly more or less than expected due to <strong>latency in motor commands</strong>. 
                        We adjusted the <strong>speed parameters</strong> and tested multiple values until we achieved accurate movement.</li>
                </ul>
            
                <h3>Reflections and Thoughts</h3>
                <p>
                    This lab provided a <strong>hands-on understanding</strong> of how <strong>robots communicate, process data, and execute precise movements</strong>. 
                    The experience of debugging and fine-tuning ROS nodes helped reinforce how real-world robotic applications require <strong>constant testing, adjustments, and optimizations</strong>.
                </p>
                <p>
                    Overall, this lab was an insightful learning experience where we <strong>learned how to control motors, capture and process camera images, and integrate various ROS components to build an efficient robotic system</strong>.
                </p>
                
            <li>
                <h3>GitHub Repository</h3>
                <p><a href="https://github.com/Sandhya-ad/my-ros-project">GitHub Repository for this Assignment</a></p>
            </li>
        </ul>
        </div>
        
    </section>

    <section class="references">
        <h2>References</h2>
        <ul>
            <li>ROS Documentation: <a href="https://wiki.ros.org">https://wiki.ros.org</a></li>
            <li>Duckietown Docs: <a href="https://docs.duckietown.com">https://docs.duckietown.com</a></li>
            <li>ChatGPT: Used to understand some topics, debug and check grammar in the report</li>
        </ul>
    </section>
        <h1>By: Sandhya Adhikari and Fumanpreet Singh</h1>

    <a href="index.html">Back to Assignments</a>
</body>
</html>
