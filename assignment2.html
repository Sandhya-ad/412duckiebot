<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assignment 2 - ROS Development and Kinematics</title>
    <link rel="stylesheet" href="templates/assignment2.css">
</head>
<body>
    <h1>Assignment 2 - ROS Development and Kinematics</h1>

    
    <section class="description">
        <p>
            This assignment focuses on understanding the <strong>Robot Operating System (ROS)</strong> and its core principles in <strong>robot kinematics</strong> and <strong>odometry</strong>. 
            We implemented ROS nodes, created a publisher and subscriber, wrote code for the duckie to move straight, reverse, rotate and drive in D.
        </p>
    </section>

    <section class="deliverables">
        <h1>Deliverables</h1>
        <ul>
            <li>
                <h2>ROS Wiki Documentation</h2>
                <div class="report">
                    <h3>1. What is a ROS Node, and what is its role in the ROS ecosystem?</h3>
                    <p>
                        A <strong>node</strong> in ROS is an executable within a ROS package that performs a specific computational task. It can be thought of as an independent module in a robotic system, 
                        responsible for a specific task such as sensor data processing, motor control, or decision-making.
                    </p>
                    <p>
                        ROS nodes enable a <strong>modular architecture</strong> where different functionalities are distributed across multiple independent processes. These nodes communicate with each
                        other using topics (publish/subscribe) or services (request/response). This structure enhances <strong>organization, scalability, and fault tolerance</strong>, ensuring that a 
                        failure in one node does not disrupt the entire system. 
                        Example: A camera node might be responsible for capturing images, while a processing node analyzes those images.
                    </p>

                    <h3>2. What are ROS topics, and how do they facilitate communication between nodes?</h3>
                    <p>
                        <strong>ROS Topics</strong> act as a communication channel that allows nodes to exchange messages asynchronously. They follow a publish-subscribe model, meaning Publishers send 
                        data to a topic and Subscribers receive data from a topic.
                    </p>
                    <p>
                        A <strong>Publisher</strong> node generates and sends data to a specific topic, while any <strong>Subscriber</strong> node listening to that topic retrieves the information.
                        Topics are loosely coupled, meaning a publisher does not need to know about subscribers and vice versa. This ensures <strong>scalability</strong> and <strong>efficiency</strong>.
                        Example: A camera node publishes images to the topic /camera/image_raw, while an object detection node subscribes to this topic to process the images.
                    </p>

                    <h3>3. What are ROS services, and how do they differ from topics?</h3>
                    <p>
                        <strong>ROS Services</strong>  are another way that nodes can communicate with each other. They provide a <strong>request-response</strong> type of communication between the nodes. 
                        Unlike topics, where a publisher continuously streams messages and subscribers listen (e.g., sensor data), whereas services operate on a request-response model where a node sends
                        a request to a service, and the service responds with the necessary information or action (e.g., requesting robot status). This synchronous interaction makes services ideal for 
                        tasks that require immediate execution and direct response.
                    </p>

                    <h3>4. What are ROS messages, and how are they structured for communication?</h3>
                    <p>
                        <strong>ROS messages</strong> define the structure of data exchanged between nodes through topics, services, and actions. They ensure different nodes can correctly interpret
                        and process the data being transmitted.
                    </p>
                    <p>
                        Example: In the case of the <strong>Duckiebot's camera sensor</strong>, the topic used to publish camera frames is: <br><code>/ROBOT_NAME/camera_node/image/compressed</code>
                    </p>
                    <p>
                        The message type used over this topic is the standard <code>sensor_msgs/CompressedImage</code>, which contains the following fields:
                    </p>
                    <ul>
                        <li><strong>std_msgs/Header header</strong>: Contains a timestamp and frame reference.</li>
                        <li><strong>string format</strong>: Specifies the format of the data, such as <code>png</code> or <code>jpeg</code>.</li>
                        <li><strong>uint8[] data</strong>: An array of bytes containing the actual image in the specified format.</li>
                    </ul>
                    <p>
                        This structured message format allows the Duckiebot to efficiently publish and process camera images, enabling vision-based tasks such as lane detection and object tracking.
                    </p>
                    

                    <h3>5. What is a ROS bag, and how is it used in data recording and playback?</h3>
                    <p>
                        A <strong>ROS bag</strong> is a file format for recording and playback ROS topics. This feature is useful for <strong>debugging, analysis, and simulation</strong> of
                        robotic behavior without needing real hardware.  
                    </p>
                    <p>
                        Writing a rosbag file:
                        <ul>
                            <li>Open a terminal and execute: <code>dts start_gui_tools</code></li>
                            <li>Run: <code>rosbag record topic -O move.bag</code>, where <code>topic</code> is your wheel command topic.</li>
                            <li>Open a separate terminal and execute your node using: <code>dts devel run -R csc229XX -L ....</code></li>
                            <li>Once it’s finished, go back to your first terminal and press <code>Ctrl + C</code> to stop recording the bag file.</li>
                            <li>Now, you have your ROSBag file inside your Docker container. You need to move it locally to your PC.</li>
                        </ul>

                    </p>

                    <h3>6. Typical workflow for setting up communication between multiple nodes in ROS.</h3>
                    <p>
                        Assuming that you have Duckiebots which uses ROS1, and ROS is already installed on them through the Docker which was covered in Exercise 1. Now, create a new ROS-compatible DTProject
                        - an interface for using ROS with duckiebots which is available at <a href="https://github.com/duckietown/template-ros/">duckietown/template-ros</a>. 
                    </p>
                    <p>
                         Now, create a new catkin package inside a DTProject because ROS uses the catkin build system to organize and build its software. In a nutshell, catkin organizes entire projects in the so-called catkin workspaces. A catkin workspace is nothing more than a directory containing a bunch of software modules called 
                        catkin packages. Each software module can contain a set of executables (e.g., binaries, script files) called ROS nodes. ROS nodes interact with one another using two of the most
                        common communication patterns, called publish-subscribe and request-reply.

                        How and where to create a new Catkin package is available at <a href="https://docs.duckietown.com/daffy/devmanual-software/beginner/ros/catkin-packages.html#create-a-new-catkin-package">catkin package creation</a>.  
                    </p>
                    <p>
                        ROS implements the publish-subscribe pattern using ROS Publishers and ROS Subscribers. The general concept is simple, a publisher has the job of publishing messages from a ROS node into the ROS network for other nodes to receive (using ROS Subscribers).
                        How and where to create Publisher ROS Node is available at <a href="https://docs.duckietown.com/daffy/devmanual-software/beginner/ros/publisher.html">publisher node creation</a>. Similarly, for Subscribers, the general concept is simple: a subscriber has the job of listening for messages about a specific topic that
                        are published by other ROS nodes (using ROS Publishers) over a ROS network. How and where to create Subscriber ROS Node is available at <a href="https://docs.duckietown.com/daffy/devmanual-software/beginner/ros/subscriber.html">subscirber node creation</a>
                        <br>
                        We now need to the tell our file system that we want our file my_publisher_node.py (any publisher node) and my_subsciber_node.py (any subscriber node) be treated as an executable file. We do so by running the following command from the root of our DTProject:
                        <br><code>chmod +x ./packages/my_package/src/my_publisher_node.py</code>  and <code>chmod +x ./packages/my_package/src/my_subscriber_node.py</code>
                    </p>
                    <p>
                        Everything in Duckietown runs inside Docker containers. This means that we also need to tell Docker what to run when the container is started. In this case, we want our new ROS publisher node to run.
                        Each DTProject compiles into a single Docker image, but we can declare multiple start “behaviors” for the same project/image so that the same project can serve multiple (though related) purposes. We can use launchers to accomplish this, now we have create a new launcher to allow for this new start behavior.
                        <br>
                        To Launch the Publisher node or Subscriber node, run the following command on terminal: 
                        <br><code>dts devel build -H ROBOT_NAME -f</code>
                        <br><code>dts devel run -H ROBOT_NAME -L my-publisher</code>
                        
                    </p>
                    
                </div>
            </li>

            <li>
                <h2>ROS Implementation Tasks</h2></li>
            <li>
                <h3>A screenshot of subscriber info from “Hello from…”</h3>
                <div class="image-container">
                    <img src="images/publisher.png" alt="ROS Subscriber" class="deliverable-photo">
                </div>
                <p>
                    In this task, the <strong>Publisher node</strong> is responsible for sending messages to a topic named <code>chatter</code> in ROS. ROS node named my_publisher_node, which is of 
                    type NodeType.GENERIC. This node type is used for general-purpose nodes that do not fit into specialized categories like perception or control. The node retrieves the vehicle name
                    from the environment variable VEHICLE_NAME and publishes messages to the chatter topic at a rate of 1 Hz (once per second). The rospy.spin() function is included to keep the 
                    process alive, even though the run() method already handles the looping. This ensures that the node remains active and can continuously publish messages until ROS is shut down.
                </p>
                
                <p><strong>Subscriber Node (MySubscriberNode)</strong></p>
                <div class="image-container">
                    <img src="images/subscriber.png" alt="ROS Subscriber" class="deliverable-photo">
                </div>
                <p>
                    In this task, the <strong>Subscriber node</strong> listens for messages being published on a topic named <code>chatter</code> in ROS. ROS node named my_subscriber_node, which is of type 
                    NodeType.GENERIC. The GENERIC node type is used for general-purpose nodes that do not fall under specific categories like control or sensing. The node subscribes to the chatter topic,
                    which publishes messages of type String. When a new message arrives, the callback function is triggered, logging the received message. The rospy.spin() function ensures that the node
                    keeps running, continuously listening for messages on the topic and executing the callback function each time a message is received. This allows for repeated processing of incoming 
                    data without the script terminating.
                </p>
                
                <ul>
                    <li>
                        Implementing the <strong>Publisher-Subscriber model</strong> in ROS provided valuable insight into how different nodes in a robotic system 
                        communicate asynchronously.</strong>.
                    </li>
                    <li>
                        This approach allowed us to establish <strong>real-time communication</strong> between nodes, ensuring seamless data flow without dependencies. 
                        We learned how to design independent nodes that can send and receive messages autonomously, improving overall system reliability.
                    </li>
                </ul>
                
                
            <li>
                <h3>A screenshot of your robot’s camera image view in your annotated and customized
                    camera image”</h3>
                <div class="image-container">
                    <img src="images/grey.png" alt="Grey image" class="deliverable-photo">
                </div>
                <div class="report">
                    <p><strong>Explanation:</strong></p>
                
                    <p>
                        The <strong>camera_reader_node</strong> is a ROS visualization node that processes real-time image data from the Duckiebot’s front-facing camera. 
                        It subscribes to the topic <code>/ROBOT_NAME/camera_node/image/compressed</code>, which continuously receives compressed image frames in ROS format 
                        (<code>sensor_msgs/CompressedImage</code>). Using CvBridge, the node converts the compressed ROS image into an OpenCV-compatible format, enabling efficient image processing.
                    </p>
                
                    <p>
                        Once decoded, the color image is converted into a grayscale image using <code>cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</code>, removing color information while preserving 
                        essential intensity details. Grayscale processing is widely used in computer vision as it reduces computational complexity while maintaining crucial edge and shape information. 
                        The node then overlays annotated text on the processed image using <code>cv2.putText()</code>, positioning it near the bottom-left corner in green.
                    </p>
                
                    <p>
                        After processing, the modified grayscale image is republished to the custom topic <code>/ROBOT_NAME/camera_processed/image/compressed</code>, allowing other ROS nodes to access 
                        and utilize the transformed data. The callback function ensures continuous image processing, while <code>rospy.spin()</code> keeps the node running, actively listening for new image messages.
                        <br>
                        Additionally, the node <strong>publishes this new annotated image on a custom topic</strong>, enabling other nodes 
                        to subscribe and process the modified image. To visualize the output, we use <strong>rqt_image_view</strong>, 
                        a ROS tool that allows real-time viewing of image topics. Using <code>rqt_image_view</code>, we can display 
                        the camera feed from our custom topic and ensure that the image processing steps, such as grayscale conversion 
                        and annotation, are correctly applied.
                    </p>
                </div>
                
            </li>

            <li>
                <h3>A video of the Duckiebot moving forward and backward for 1.25 meters</h3>
                <video width="auto" height="600" controls>
                    <source src="videos/straight_code.mov" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <div class="report">
                    <p>
                        The WheelEncoderReaderNode is implemented as a subclass of DTROS, a specialized ROS class designed for Duckietown robots. This node is assigned the NodeType.PERCEPTION
                        classification, which is appropriate for processing sensor data, such as wheel encoder readings. The selection of this node type enables accurate tracking of the 
                        Duckiebot’s movement in both forward and reverse directions by leveraging encoder feedback. The node subscribes to two topics, /left_wheel_encoder_node/tick and
                        /right_wheel_encoder_node/tick, which provide real-time wheel encoder data using the WheelEncoderStamped message type. The callback_left and callback_right functions
                        are employed to process data from each encoder independently. Using separate callbacks for each wheel ensures that encoder readings are handled correctly, accommodating
                        differences in resolution or synchronization issues between the left and right wheels.
                    </p>
                    <p>
                        Additionally, the node publishes commands to /wheels_driver_node/wheels_cmd using the WheelsCmdStamped message type to control wheel velocities. A rate of rospy.Rate(1)
                        is used for command execution, ensuring updates are sent once per second, which helps in precise movement execution while preventing excessive command flooding. The logic 
                        behind movement control is based on encoder ticks, with a threshold of 750 ticks used to drive the Duckiebot forward for 1.25 meters and then backward for 1.25 meters. The
                        radius of the wheel is obtained from <code>rospy.get_param(f"/{self._vehicle_name}/kinematics_node/radius")</code> which comes out to be 0.032m which covers distance of almost
                        20cm which is 135 ticks. To cover the distance of 1.25m, it requires nearby 840 ticks (calculated) which was off by almost 90 ticks as measured by measuring tape in real time.
                    </p>
                    <p>
                        The parameters for movement are set using THROTTLE_LEFT and THROTTLE_RIGHT with directional multipliers (DIRECTION_LEFT, DIRECTION_RIGHT), allowing the Duckiebot to switch between 
                        moving forward and backward seamlessly. This approach ensures accurate motion tracking and enables controlled movements such as straight-line driving and rotations based on
                        encoder feedback. The use of encoder-based tick counting allows for deterministic control, crucial for tasks like calibration and precise navigation.
                    </p>

                    <ul>
                        <li>
                            <strong>Why is there a difference between the actual and desired location?</strong>
                            <ul>
                                <li>Wheel slippage and uneven surfaces can cause discrepancies.</li>
                                <li>Encoder inaccuracies may lead to slight deviations.</li>
                                <li>Latency in command execution affects precise movement.</li>
                                <li>Variations in motor response time can contribute to errors.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>What speed did you use?</strong>
                            <ul>
                                <li>The speed was set using <code>THROTTLE_LEFT = 0.545</code> and <code>THROTTLE_RIGHT = 0.5</code>.</li>
                                <li>Each wheel runs at approximately 50% throttle. The minor difference between left and right throttle is maintain the straight line motion.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>What happens when you increase or decrease the speed?</strong>
                            <ul>
                                <li><strong>Increasing speed:</strong> The Duckiebot reaches the target distance faster but may overshoot the distance and a little jerk during transition phase can deflect it.</li>
                                <li><strong>Decreasing speed:</strong> The Duckiebot moves more slowly, improving precision, but it may struggle to overcome small obstacles or get stuck due to lower momentum.</li>
                            </ul>
                        </li>
                    </ul>
                </div>
                
            </li>
            <li>
                <h3>A video of the Duckiebot rotating clockwise and counter clockwise</h3>
                <video width="auto" height="600" controls>
                    <source src="videos/rotate.mov" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <div class="report">
                    <p>
                        The WheelEncoderReaderNode is implemented as a subclass of DTROS, a specialized ROS class designed for Duckietown robots. This node is assigned the NodeType.PERCEPTION classification, which is ideal for processing sensor data, such as wheel encoder readings. Choosing this node type is crucial for ensuring precise movement tracking, particularly for tasks requiring accurate control of the Duckiebot’s orientation, such as rotating 90 degrees clockwise and counterclockwise. The node subscribes to two topics, /left_wheel_encoder_node/tick and /right_wheel_encoder_node/tick, which provide real-time wheel encoder data using the WheelEncoderStamped message type. These messages contain essential information such as encoder resolution, tick count, and direction, allowing the system to monitor movement with high accuracy.
                        <br>
                        In addition to subscribing to encoder data, the node publishes wheel commands to /wheels_driver_node/wheels_cmd using the WheelsCmdStamped message type. This allows direct control over wheel velocities, enabling the Duckiebot to execute controlled movements. The use of separate callback functions, callback_left and callback_right, ensures that each wheel's data is processed independently. This approach accounts for differences in resolution and synchronization between the two wheels, which is vital for maintaining balanced and predictable motion. The implementation of two distinct callbacks prevents data loss and improves the reliability of movement tracking.
                        <br>
                        The rospy.Rate(1) command ensures that commands are published at a rate of 1 Hz, meaning one update per second. This controlled rate prevents excessive command flooding while maintaining smooth motion execution. Another rate, rospy.Rate(20), is used for processing messages at a higher frequency to ensure timely updates.
                    </p>

                    <p>
                        The movement logic is based on encoder tick counting. The Duckiebot rotates 90 degrees clockwise when the left wheel moves forward while the right wheel remains stationary. This is achieved by setting THROTTLE_LEFT = 0.545 and THROTTLE_RIGHT = 0, ensuring that only the left wheel moves. The bot completes the rotation after registering 35 encoder ticks which was calculated using wheel base of bot that is 12cm (measured using tape) which make a circle of circumference 20.11cm. In order to rotate 90 degrees clockwise, the wheel of bot with radius 3.2cm (135 ticks in one rotation) has to cover almost 5cm which turns out to be 35 ticks. The left wheel moves backward while the right wheel remains stationary. This is done by setting DIRECTION_LEFT = -1, effectively reversing the left wheel’s movement while maintaining the same tick threshold of 35.
                    </p>
                    <p>
                        <h3>i. Did you observe any deviations in the rotation?</h3>
                        <p>Yes, there might be slight deviations in the rotation of the Duckiebot.</p>
                        
                        <h3>ii. If deviations exist, what could be the cause?</h3>
                        <ul>
                            <li><strong>Wheel Slippage:</strong> The wheels might slip, especially on surfaces with low traction, affecting the precise movement and leading to deviations in rotation.</li>
                            <li><strong>Encoder Resolution Differences:</strong> If the encoder resolution between the left and right wheels is not perfectly matched or calibrated, it could cause unequal movement, resulting in rotational inaccuracies.</li>
                            <li><strong>Motor Inconsistencies:</strong> Differences in motor performance or power output between the left and right motors could result in uneven wheel speeds, leading to rotational drift.</li>
                            <li><strong>Tick Counting Errors:</strong> Errors in counting the encoder ticks due to sensor noise or incorrect message handling could result in an inaccurate number of ticks being counted, causing deviations in rotation.</li>
                            <li><strong>Encoder Calibration Issues:</strong> If the wheel encoders are not calibrated correctly, the number of ticks per wheel revolution might not correspond accurately to the actual distance traveled or rotation achieved.</li>
                            <li><strong>Asymmetry in the Chassis:</strong> Structural misalignment or differences in the positioning of the wheels on the chassis could lead to an uneven turning radius, resulting in deviations during rotation.</li>
                            <li><strong>Environmental Factors:</strong> External factors such as uneven surfaces, obstacles, or floor conditions could affect the robot’s movement and cause deviations in the expected rotation.</li>
                        </ul>
                        <h3> Does your program exit properly after you nish all the above tasks?</h3>
                        <p>Yes, the program exits properly after completing all the tasks. Once the desired rotations are finished and the tick count condition is met, the bot stops by publishing a zero-velocity command to the wheels. This ensures that the Duckiebot is stabilized before the program ends. Afterward, a clean shutdown procedure is initiated with the <code>rospy.signal_shutdown()</code> function, which gracefully shuts down the node. This release of resources ensures that no processes or subscriptions are left running, and the program terminates without leaving any residual tasks.</p>
                    </p>
                </div>
                
            </li>
            <li>
                <h3>ROSBag Plot</h3>
                <div class="image-container">
                    <img src="images/straight_plot.png" alt="ROSBag Plot" class="deliverable-photo">
                </div>
                
                <div class="report">
                    <h4>How We Plotted the ROSBag Data</h4>
                    <p>
                        To analyze the motion of the Duckiebot, we recorded the relevant encoder messages in a ROSBag file while executing 
                        straight-line motion and rotation tasks. The following steps were followed:
                    </p>
                    <ul>
                        <li>We ran the command to start recording ROSBag data:
                            <br><code>rosbag record -O move.bag /ROBOT_NAME/wheel_driver_node/wheels_cmd</code></li>
                        <li>Executed the movement command while ROSBag was actively recording.</li>
                        <li>Once the movement was completed, we stopped recording using <code>Ctrl + C</code>.</li>
                        <li>Extracted and analyzed the recorded data by replaying the bag file.</li>
                        <li>Used a Python script to extract encoder readings and plot the motion trajectory.</li>
                    </ul>
            
                    <h4>What the ROSBag Plot Shows</h4>
                    <p>
                        The plot generated from the ROSBag data provides a visualization of the Duckiebot’s movement based on **wheel encoder ticks**.
                        It shows:
                    </p>
                    <ul>
                        <li>The <strong>distance traveled</strong> based on encoder readings.</li>
                        <li>Variations in motion, including small deviations due to motor differences.</li>
                        <li>The effect of different wheel speeds on trajectory accuracy.</li>
                    <p>
                        From the plot, we could observe slight deviations due to latency in motor commands, uneven flooring**, and 
                        wheel friction differences**, which were later fine-tuned for better accuracy.
                    </p>
            
                    <h4>Reference</h4>
                    <ul>
                        <li>ChatGPT: Used for clarifications on ROSBag recording, plotting methods, and general debugging.</li>
                    </ul>
                </div>
            </li>
            

            <div class="report">
                <h2>Straight-Line Motion and Rotation Task Explanation</h2>
            
                <h3>1. Straight-Line Motion</h3>
                <p>
                    The Duckiebot was programmed to move forward for <strong>1.25 meters (125 cm)</strong> and then reverse back the same distance. 
                    While the movement was mostly accurate, some deviations were observed between the <strong>actual and desired location</strong> 
                    due to minor variations in wheel speed and latency in stopping.
                </p>
                
                <h4>Mathematical Calculations for 125 cm Distance</h4>
                <p>
                    To ensure the Duckiebot traveled exactly <strong>125 cm</strong>, we calculated the required number of <strong>encoder ticks</strong> 
                    based on our measured values.
                </p>
                
                <ul>
                    <li>
                        The measured <strong>wheel circumference</strong> is <strong>20.1 cm</strong>.
                    </li>
                    <li>
                        From our measurements, we found that <strong>one full wheel rotation (almost 20 cm traveled) corresponds to 135 encoder ticks</strong>.
                    </li>
                    <li>
                        To determine how many <strong>ticks correspond to 1 cm</strong>, we calculated:
                        <br>
                        <code>Ticks per cm = 135 / 20 ≈ 6.43 ticks/cm</code>
                    </li>
                    <li>
                        To find the number of <strong>ticks needed to travel 125 cm</strong>, we used:
                        <br>
                        <code>Required Ticks = (135 / 20.1) * 125 ≈ 840 ticks</code>
                    </li>
                </ul>
                
                <p>
                    Thus, the Duckiebot needed to move approximately <strong>840 ticks forward</strong> and then <strong>840 ticks in reverse</strong>.
                    To account for the motor command delay, we have to reduce the few ticks and set it to 750 which turns out to be the ticks required to travel 125cm (measured using tape).
                </p>
                <p>
                    Initially, we set equal throttle values for both wheels, but the robot veered slightly to the left. To correct this, we adjusted the 
                    <strong>left wheel throttle to 0.545</strong> and the <strong>right wheel throttle to 0.5</strong>, ensuring a straighter trajectory.
                </p>
            
            
                <h3>2. Rotation Task</h3>
                <p>
                    The Duckiebot was programmed to rotate <strong>90 degrees clockwise (π/2 radians)</strong> and then rotate back to 
                    <strong>0 degrees counterclockwise</strong>. The movement was based on **encoder tick counts**, ensuring precise rotation.
                </p>

                    <h4>Mathematical Calculations for 90-Degree Rotation</h4>
                    <p>
                        To rotate the Duckiebot by <strong>90 degrees</strong>, we needed to determine the number of <strong><ticks/strong> required. 
                    </p>

                    <ul>
                        <li>
                            From previous measurements, we determined that <strong>one full wheel rotation corresponds to 135 encoder ticks</strong>.
                        </li>
                        <li>
                            We found that rotating 90 degrees required the <strong>left wheel to move forward for 35 ticks</strong> 
                            while the right wheel remained stationary.
                        </li>
                        <li>
                            The formula used for determining tick count:
                            <br>
                            <code>Required Ticks for 90° Rotation = 1/4 of circumference of cirle with radius 12cm (wheel base of bot) = 20.11 / 4 ≈ 5cm</code>
                            <code>135 ticks to make one rotation by wheel of radius 3.2cm, in order to cover 5cm by wheels it requires 35 ticks</code>
                        </li>
                        <li>
                            To rotate back counterclockwise to <strong>0 degrees</strong>, the <strong> wheel moved in reverse for the same 35 ticks</strong>
                        </li>
                    </ul>

                    <p>
                        Thus, the Duckiebot performed an accurate <strong>90-degree clockwise rotation</strong> and then returned to its original position 
                        by rotating <strong>90 degrees counterclockwise</strong>, both using <strong>35 encoder ticks per movement</strong>.
                    </p>

                    <p>
                        Initially, small deviations were observed where the bot sometimes <strong>rotated slightly more than 90 degrees</strong>. 
                        To correct this, adjustments were made by fine-tuning the motor power and ensuring the **right wheel remained completely still** 
                        during the rotation.
                    </p>
            </div>
            

            <li>
                <h3>A video of the Duckiebot driving in a “D” shape</h3>
                <video width="auto" height="600" controls>
                    <source src="videos/IMG_3980.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <video width="auto" height="600" controls>
                    <source src="videos/IMG_3982.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <div class="report">
                    <p>The Duckiebot followed a <strong>“D” shaped trajectory</strong> using wheel encoder odometry.</p>
                    
                    <h4>LED State Indicators</h4>
                    <ul>
                        <li><strong>🔴 Red LED:</strong> Indicates that the Duckiebot is <strong>stopping for 5 seconds</strong>.</li>
                        <li><strong>🔵 Blue LED:</strong> Indicates that the Duckiebot is <strong>moving along the D-shaped path</strong>.</li>
                    </ul>
            
                    <p>
                        The LED color transitions help visualize the different motion states of the Duckiebot, ensuring clear feedback 
                        on when it is actively moving versus when it is in a stationary state.
                    </p>
                    <h4>LED State Functionality</h4>
                        <p>
                            The LED indicators provide <strong>visual feedback</strong> on the Duckiebot’s motion states throughout the execution of the <strong>D-shaped path</strong>.
                        </p>

                        <ul>
                            <li><strong>🔳 White LED (Startup Phase - 2 seconds):</strong>
                                <ul>
                                    <li>At the very beginning, the LEDs turn <strong>white for 2 seconds</strong>.</li>
                                    <li>This ensures that if the Duckiebot’s LEDs were already <strong>red</strong>, we can still observe the transition when stopping later.</li>
                                </ul>
                            </li>

                            <li><strong>🔴 Red LED (Stop - 5 seconds):</strong>
                                <ul>
                                    <li>The Duckiebot stops and holds its position for <strong>5 seconds</strong>.</li>
                                    <li>The LEDs turn <strong>red</strong>, indicating that the robot is in a stationary state.</li>
                                </ul>
                            </li>

                            <li><strong>🔵 Blue LED (Moving - Tracing the "D" Shape):</strong>
                                <ul>
                                    <li>As the Duckiebot starts moving to trace the <strong>D-shaped path</strong>, the LEDs turn <strong>blue</strong>.</li>
                                    <li>This helps indicate that the robot is actively navigating the trajectory.</li>
                                </ul>
                            </li>

                            <li><strong>🔴 Red LED (Final Stop - 5 seconds):</strong>
                                <ul>
                                    <li>At the <strong>end of the "D" shape</strong>, when the Duckiebot completes its rotation and returns to the starting position, the LEDs turn <strong>red again</strong>.</li>
                                    <li>The robot stops for <strong>another 5 seconds</strong> before finishing the task.</li>
                                </ul>
                            </li>
                        </ul>

                        <p>
                            This LED transition system provides a <strong>clear visual representation</strong> of when the Duckiebot is <strong>starting, stopping, and moving</strong>, 
                            making it easier to track its current state at any given moment.
                        </p>
                    
                    <h4>Tracing "D"</h4>
                    <p>
                        To achieve Duckiebot's trajectory control, several components and functions are used to manage the robot's movement and states, utilizing callback functions and node types effectively. The node is defined with the type NodeType.PERCEPTION, which indicates that the node is primarily responsible for processing sensory input (such as encoder data from the wheels) to control the robot's movement. The callback functions, callback_left and callback_right, listen to the left and right wheel encoder topics, respectively. These functions process data received from wheel encoders, updating internal variables like _ticks_left and _ticks_right, which track the position of the wheels. The callback functions are critical in ensuring the robot's behavior is adaptive. By using separate callbacks for each wheel, the system can independently track the progress of each wheel, adjusting the robot's trajectory if discrepancies between the two wheels occur. The use of callbacks here ensures real-time responsiveness, allowing the robot to adjust its behavior dynamically as it receives data about its wheel movement.
                    </p>
                    <p>
                        The state logic is implemented in two main states: state_1 and state_2 (state_2 includes state_3). State 1 is designed to keep the robot stationary for 5 seconds while setting the LED to red, effectively halting all movement and ensuring the robot is in a known state before starting the path. In State 2, the robot begins a trajectory along a "D" shaped path. It updates the wheel velocities based on encoder ticks, which represent the movement of the wheels. The robot moves forward for a specific distance (1.2 meters), stops, rotates by 90 degrees clockwise, and then proceeds along other segments of the path. During this process, the encoder values are continuously monitored in the callbacks, ensuring that the robot moves the exact distance required. These values are used to stop the robot when the target distance is achieved. The states are designed to control the robot's motion step-by-step, using feedback from the encoders to ensure precision in distance and rotational movement. This feedback loop ensures that the robot operates reliably, adjusting its motion based on real-time sensor data.
                    </p>
                    <h2>1. Move straight for 1.2m</h2>
                    <p>The robot starts by moving straight for 1.2 meters. It uses the following velocity parameters:</p>
                    <ul>
                        <li><strong>vel_left:</strong> 0.33 (50% throttle forward)</li>
                        <li><strong>vel_right:</strong> 0.33 (50% throttle forward)</li>
                    </ul>
                    <p>The initial tick count is recorded, and a loop is used to track the difference between the current and initial ticks. The robot moves forward until both left and right ticks reach 720, which corresponds to 1.2 meters of movement.</p>
                    
                    <h2>2. Rotation 90 Degrees Clockwise</h2>
                    <p>After moving straight, the robot rotates 90 degrees clockwise using the following parameters:</p>
                    <ul>
                        <li><strong>THROTTLE_LEFT:</strong> 0.5 (50% throttle forward)</li>
                        <li><strong>THROTTLE_RIGHT:</strong> 0 (stopped)</li>
                    </ul>
                    <p>The robot is set to rotate by publishing a message that adjusts the left wheel's speed. It rotates until the left wheel ticks change by 99 ticks, corresponding to a 90-degree turn.</p>
                
                    <h2>3. Move Straight for 0.92m</h2>
                    <p>Next, the robot moves forward for 0.92 meters using the following parameters:</p>
                    <ul>
                        <li><strong>vel_left:</strong> 0.33 (50% throttle forward)</li>
                        <li><strong>vel_right:</strong> 0.30 (50% throttle forward)</li>
                    </ul>
                    <p>The initial ticks are recorded again, and the robot continues to move straight until both left and right ticks reach 500, completing 0.92 meters.</p>
                
                    <h2>4. Curve Right</h2>
                    <p>The robot now makes a right curve using the following parameters:</p>
                    <ul>
                        <li><strong>THROTTLE_LEFT:</strong> 0.47 (50% throttle forward)</li>
                        <li><strong>THROTTLE_RIGHT:</strong> 0.2 (reduced throttle)</li>
                    </ul>
                    <p>The robot's wheels are set to different speeds, and it curves right by moving forward until the left wheel ticks reach 240 and the right wheel ticks reach 120, completing the curve.</p>
                
                    <h2>5. Move Straight for 0.61m</h2>
                    <p>The robot moves forward again for 0.61 meters using the following parameters:</p>
                    <ul>
                        <li><strong>vel_left:</strong> 0.33 (50% throttle forward)</li>
                        <li><strong>vel_right:</strong> 0.30 (50% throttle forward)</li>
                    </ul>
                    <p>Once again, the robot tracks its movement based on the ticks, moving straight until it reaches the required tick count.</p>
                
                    <h2>6. Final Curve Right</h2>
                    <p>The robot makes a final curve right with the following parameters:</p>
                    <ul>
                        <li><strong>THROTTLE_LEFT:</strong> 0.48 (50% throttle forward)</li>
                        <li><strong>THROTTLE_RIGHT:</strong> 0.2 (reduced throttle)</li>
                    </ul>
                    <p>Similar to the previous curve, the robot moves until the left wheel ticks reach 240 and the right wheel ticks reach 120, completing the turn.</p>
                
                    <h2>7. Final Movement and Rotation</h2>
                    <p>The robot moves forward for 0.92 meters and then performs another 90-degree clockwise rotation using the same parameters as the previous rotation. It stops once the final rotation is complete.</p>
                
                    <h2>Conclusion</h2>
                    <p>After completing the full path tracing the letter "D", the robot stops and changes the LED color to red, signaling the end of the path tracing process.</p>

                    
                </div>
            </li>
            
            <div class="report">
                <h2>Overall Implementation Summary</h2>
            
                <h3>What We Implemented</h3>
                <p>
                    In this lab, we implemented three key components of <strong>ROS-based robotic communication and control</strong>:
                </p>
                <ul>
                    <li>A <strong>Publisher-Subscriber model</strong>, where the publisher continuously sent messages, and the subscriber listened and logged the received data.</li>
                    <li>A <strong>Camera Processing Node</strong>, which converted live camera feed images to grayscale, added annotations, and republished them for visualization.</li>
                    <li>A <strong>Motion Control Task</strong>, where the Duckiebot moved forward and backward for a set distance and performed a <strong>90-degree rotation</strong>.</li>
                </ul>
            
                <h3>How It Works</h3>
                <p>
                    The <strong>Publisher-Subscriber model</strong> demonstrated <strong>asynchronous communication</strong>, allowing different nodes to send and receive messages independently. 
                    The <strong>camera node</strong> processed images in real-time using <strong>OpenCV</strong>, while the <strong>motion task</strong> involved executing precise motor commands to control the Duckiebot’s movement.
                </p>
            
                <h3>What We Learned</h3>
                <ul>
                    <li>How <strong>ROS topics</strong> facilitate real-time message exchange between independent nodes.</li>
                    <li>The importance of <strong>asynchronous messaging</strong> for scalable robotic systems.</li>
                    <li>How <strong>image processing</strong> in ROS works using OpenCV and how images can be modified and republished.</li>
                    <li>The challenges of <strong>accurate movement control</strong> and how to fine-tune parameters for better results.</li>
                </ul>
            
                <h3>Challenges and How We Overcame Them</h3>
                <ul>
                    <li><strong>Motion Deviations:</strong> The Duckiebot sometimes moved slightly more or less than expected due to <strong>latency in motor commands</strong>. 
                        We adjusted the <strong>speed parameters</strong> and tested multiple values until we achieved accurate movement.</li>
                </ul>
            
                <h3>Reflections and Thoughts</h3>
                <p>
                    This lab provided a <strong>hands-on understanding</strong> of how <strong>robots communicate, process data, and execute precise movements</strong>. 
                    The experience of debugging and fine-tuning ROS nodes helped reinforce how real-world robotic applications require <strong>constant testing, adjustments, and optimizations</strong>.
                </p>
                <p>
                    Overall, this lab was an insightful learning experience where we <strong>learned how to control motors, capture and process camera images, and integrate various ROS components to build an efficient robotic system</strong>.
                </p>
                
            <li>
                <h3>GitHub Repository</h3>
                <p><a href="https://github.com/Sandhya-ad/my-ros-project">GitHub Repository for this Assignment</a></p>
            </li>
        </ul>
        </div>
        
    </section>

    <section class="references">
        <h2>References</h2>
        <ul>
            <li>ROS Documentation: <a href="https://wiki.ros.org">https://wiki.ros.org</a></li>
            <li>Duckietown Docs: <a href="https://docs.duckietown.com">https://docs.duckietown.com</a></li>
            <li>ChatGPT: Used to understand almost all of topics to get more details (We were running out of time), debug and check grammar in the report</li>
        </ul>
    </section>
        <h1>By: Sandhya Adhikari and Fumanpreet Singh</h1>

    <a href="index.html">Back to Assignments</a>
</body>
</html>
