<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assignment 3</title>
    <link rel="stylesheet" href="templates/assignment3.css">
</head>
<body>
    <h1>Assignment 3 </h1>
    <section class="deliverables">
        <h1>Part 1: Computer Vision</h1>
        <ul>
            <li>
                A screen capture of the distorted camera image
                <div class="image-container">
                    <img src="images/distorted.png" alt="Distorted image" class="deliverable-photo">
                </div>
                <div class="report">
                    <h2>Camera Distortion</h2>
                    <p>
                        Camera distortion occurs due to the curvature of the lens, which bends light unevenly as it enters the camera sensor. This effect is particularly noticeable in radial distortion, where straight lines appear curved, especially near the edges of the image. We can see the same thing happen to our camera image in the above picture, especially near the edges, lines appear bent
                        </p>
                    <p>
                        <strong>Subscribing to camera topic: </strong>
                        To capture a distorted image, we subscribe to the ROS topic /camera_node/image/compressed, which provides a compressed stream of images from the camera. The subscription allows us to receive and process real-time camera data in ROS.
                    </p>
                </div>
            </li>

            <li>
                A screen capture of the undistorted camera image
                <div class="image-container">
                    <img src="images/undistorted_blur.png" alt="Distorted image" class="deliverable-photo">
                </div>
                <div class="report">
                    <h2>Undistorting the Image</h2>
                    <p>
                        To correct lens distortion, we use <strong> intrinsic parameters</strong> that describe the camera’s internal properties, such as focal length, optical center, and distortion coefficients. These parameters allow us to mathematically reverse the distortion effect and obtain a properly aligned image.
                    </p>
                    <p>
                        By performing these steps, we ensure that the camera feed provides an accurate and geometrically correct 
                        representation of the environment, which is essential for tasks such as color detection and lane following.
                    </p>
                    
                    <h3>What we did</h3>
                    
                    <ul>
                        <li>
                            <strong>Subscribing to the camera topic:</strong>
                            The robot receives real-time images from the ROS topic <code>/camera_node/image/compressed</code>. 
                            These images contain lens distortions that need correction before they can be used for accurate perception.
                        </li>
                        
                        <li>
                            <strong>Retrieving camera intrinsic parameters:</strong>
                            The robot's camera has unique calibration parameters stored in <code>/camera_node/camera_info</code>. 
                            These include:
                            <ul>
                                <li><strong>Camera Matrix:</strong> Defines how the camera maps the 3D world to a 2D image.</li>
                                <li><strong>Distortion Coefficients:</strong> Corrects radial and tangential distortions.</li>
                            </ul>
                            Using these parameters ensures that undistortion is tailored specifically for the robot's camera.
                        </li>
                        
                        <li>
                            <strong>Calculating an optimal new camera matrix:</strong>
                            Using <code>cv2.getOptimalNewCameraMatrix()</code>, we refine the camera matrix to minimize unwanted black borders 
                            in the corrected image. This step improves the visual quality of the output.
                        </li>
                    
                        <li>
                            <strong>Applying image correction:</strong>
                            The <code>cv2.undistort()</code> function is used to transform the distorted image into an undistorted one. 
                            This ensures that straight lines appear as they should, and objects are accurately represented.
                        </li>
                    
                        <li>
                            <strong>Publishing the corrected image:</strong>
                            The final undistorted image is then published to <code>/camera_undistorted/image/compressed</code>. 
                            This allows other ROS nodes, such as those responsible for <strong>color detection and lane following</strong>, to work 
                            with a geometrically correct image.
                        </li>
                    </ul>
                    
                    <h3>Purpose:</h3>

                    <p>
                        Undistortion is essential for accurate lane detection and navigation. When an image is distorted, straight lane markings can appear curved, leading to incorrect edge detection and misalignment. By correcting distortion, the robot can accurately perceive lane boundaries and navigate accordingly. Additionally, distorted images can stretch or compress objects, causing inconsistencies in object sizing. Undistortion ensures that objects appear with correct proportions, improving the accuracy of both lane detection and obstacle recognition. Color detection also benefits from undistortion, as color inconsistencies often occur near the edges of distorted images. By stabilizing the image, the robot can reliably detect lane colors without interference from warped color transitions. Finally, distance estimation depends on an accurate visual representation of the environment. Since distorted images alter the perceived shape and position of objects, correcting the distortion ensures that distance calculations remain precise, leading to more reliable navigation and decision-making.
                    </p>
                    
                    <ul>
                        <li>
                            <h2>Lane Detection Contours</h2>
                            <p>
                                The images below show the detected contours for <strong>blue, red, and green lanes</strong> using 
                                our ROS subscriber in <code>rqt_image_view</code>. The contours are drawn based on 
                                <strong>color segmentation and edge detection</strong>, ensuring accurate lane identification.
                            </p>
            
                            <h3>Contours Detecting a Blue Line</h3>
                            <div class="image-container">
                                <img src="images/blue_detection.png" alt="Blue Lane Contours" class="deliverable-photo">
                            </div>
                            <p>
                                The above image shows how the <strong>blue lane</strong> is detected. The contours are drawn 
                                around the largest detected blue area using <strong>HSV filtering and contour extraction</strong>.
                            </p>
            
                            <h3>Contours Detecting a Red Line</h3>
                            <div class="image-container">
                                <img src="images/red_detection.png" alt="Red Lane Contours" class="deliverable-photo">
                            </div>
                            <p>
                                This image demonstrates the detection of the <strong>red lane</strong>. Since red appears in two 
                                sections of the <strong>HSV color space</strong>, we use two separate hue ranges to ensure complete 
                                detection. The detected lane is highlighted with contours.
                            </p>
            
                            <h3>Contours Detecting a Green Line</h3>
                            <div class="image-container">
                                <img src="images/green_detection.png" alt="Green Lane Contours" class="deliverable-photo">
                            </div>
                            <p>
                                The <strong>green lane</strong> is detected by applying a mask within the green HSV range. The 
                                contours drawn on the detected area highlight the extracted lane.
                            </p>
                        </li>
            
                        <li>
                            <h2>Explanation of Color Detection</h2>

                            <h3>Preprocessing the Image</h3>
                            <p>
                                The camera feed is first undistorted to correct for lens distortion. 
                                Then, the image is converted from </strong>BGR to HSV format</strong> to separate 
                                color components, making color segmentation more reliable.
                            </p>
                            <p>
                                Color detection is performed by converting the <strong>BGR image to HSV format</strong> and 
                                applying predefined HSV thresholds. The <strong>HSV (Hue, Saturation, Value) color model</strong> 
                                is preferred because it separates color intensity from brightness, making it more 
                                reliable under different lighting conditions.
                            </p>
            
                            <h2>How Color Detection Works</h2>
                            <h3> Applying Color Thresholding</h3>
                            <p>
                                The HSV model is used to filter out specific colors corresponding to 
                                <strong>red, blue, and green lanes</strong>. The following ranges are applied:
                            </p>
                            <ul>
                                <li><strong>Red Lane:</strong> <code>Lower: [0, 100, 100] & [170, 100, 100], Upper: [10, 255, 255] & [180, 255, 255]</code></li>
                                <li><strong>Blue Lane:</strong> <code>Lower: [100, 150, 100], Upper: [130, 255, 255]</code></li>
                                <li><strong>Green Lane:</strong> <code>Lower: [46, 50, 65], Upper: [95, 196, 199]</code></li>
                            </ul>
                            <p>
                                Each color mask isolates only the pixels matching the specified HSV range, 
                                filtering out everything else.
                            </p>
            
                            <h3>3. Removing Noise</h3>
                            <p>
                                Morphological operations, such as </strong>closing</strong>, are applied to remove 
                                small artifacts that could cause false detections.
                            </p>
            
                            <h3>4. Finding and Drawing Contours</h3>
                            <p>
                                Using <code>cv2.findContours()</code>, lane boundaries are detected 
                                and enclosed in a bounding box using <code>cv2.boundingRect()</code>. 
                                The largest detected contour for each color is selected.
                            </p>
            
                            <h3>5. Estimating Distance to the Lane</h3>
                            <p>
                                The width of the detected lane in pixels is used to estimate its 
                                real-world distance using the <strong>pinhole camera model</strong>:
                            </p>
                            <pre>
    distance_m = (known_object_width_m * focal_length_px) / object_pixel_width
                            </pre>
                            <p>
                                The known width of the lane is a predefined constant, and the 
                                focal length is retrieved from the camera’s intrinsic parameters.
                                This is used to see how far the object is from the robot
                            </p>
            
                            <h3>6. Publishing the Detected Lane</h3>
                            <p>
                                The processed image, with detected lanes and distance labels, 
                                is published to <code>/lane_detection/image/compressed</code> for 
                                integration with navigation modules.
                                The dimensions of the detected lane is also published in the terminal.
                                Example of our output:
                                <pre>
    [INFO] [1740622024.705454]: Published lane detection image.
    [INFO] [1740622024.751194]: Detected: red, Dimensions: (240, 23), Distance: 0.223m
    [INFO] [1740622024.772927]: Published lane detection image.
    [INFO] [1740622024.830412]: Detected: red, Dimensions: (242, 22), Distance: 0.221m
                                </pre>

                            </p>
                        </li>
            
                        <li>
                            <h2>Summary of Detection for Each Color</h2>
            
                            <h3>Red Lane Detection</h3>
                            <p>
                                - <strong>Uses two HSV ranges</strong> to capture different red shades.
                                - Filters out yellow regions that may overlap with red.
                                - Extracts the <strong>largest contour</strong> and highlights it.
                                - <strong>Used to detect stop points</strong> and straight movement areas.
                            </p>
            
                            <h3>Blue Lane Detection</h3>
                            <p>
                                - Detects blue lanes using a <strong>tight HSV range</strong>.
                                - Filters out shadows and dark patches.
                                - Contours are enclosed in a bounding box.
                                - <strong>Used to trigger right turns</strong> in the navigation system.
                            </p>
            
                            <h3>Green Lane Detection</h3>
                            <p>
                                - Filters out unwanted environmental colors.
                                - Detects <strong>left-turn trigger points</strong>.
                                - Bounding boxes are drawn around the detected lane.
                            </p>
                        </li>
                    </ul>
                        </li>
            
                        <li>
                            <h2>How to Tune HSV Parameters</h2>
                            <p>
                                The HSV values for each lane color are adjusted to optimize detection. The tuning process involves:
                            </p>
                            <ul>
                                <li>
                                    <strong>Increasing or decreasing the Hue (H) range</strong> to fine-tune color recognition. 
                                    If a color is not detected properly, the hue range is <strong>expanded or shifted</strong>.
                                </li>
                                <li>
                                    <strong>Adjusting the Saturation (S) range</strong> to control how vivid the colors need to be. 
                                    This helps filter out <strong>faded colors</strong> and background noise.
                                </li>
                                <li>
                                    <strong>Modifying the Value (V) range</strong> to adjust brightness sensitivity. Raising the 
                                    lower limit reduces <strong>false detections in dark regions</strong>.
                                </li>
                                <li>
                                    <strong>Testing under different lighting conditions</strong> to ensure the values work consistently.
                                </li>
                            </ul>
                            <p>
                                The final <strong>HSV thresholds</strong> used for lane detection are:
                            </p>
                            <ul>
                                <li><strong>Blue Lane:</strong> <code>Lower: [100, 150, 100], Upper: [130, 255, 255]</code></li>
                                <li><strong>Red Lane:</strong> <code>Lower: [0, 100, 100] & [170, 100, 100], Upper: [10, 255, 255] & [180, 255, 255]</code></li>
                                <li><strong>Green Lane:</strong> <code>Lower: [46, 50, 65], Upper: [95, 196, 199]</code></li>
                            </ul>
                        </li>
                    </ul>
                    <ul>
                        <li>
                            <h3>A video of the Duckiebot going straight until red line stopping and moving 30 cm ahead</h3>
                            <video width="auto" height="600" controls>
                                <source src="videos/red_lane.mp4" type="video/mp4">
                                Your 
                            </video>
                        </li>
                        <li>
                            <h3>A video of the Duckiebot going straight until blue line stopping then turning right after turning the right lights to red</h3>
                            <video width="auto" height="600" controls>
                                <source src="videos/blue_lane.mp4" type="video/mp4">
                                Your 
                            </video>
                        </li>
                        <li>
                            <h3>A video of the Duckiebot going straight until green line stopping then turning left after turning the left lights to red</h3>
                            <video width="auto" height="600" controls>
                                <source src="videos/blue_lane.mp4" type="video/mp4">
                                Your 
                            </video>
                        </li>
                    </ul>
                    <h1>Lane Behavior Execution Report</h1>
                    <section class="deliverables">
                        
                        <h2>Detecting Lane Colors</h2>
                        <p>
                            The Duckiebot detects lanes using a camera feed, which is processed in real time. The image is converted from BGR to HSV format, allowing the system to separate color information efficiently. Specific HSV thresholds are applied to filter out the target colors:
                        </p>
                        <ul>
                            <li><strong>Red Lane:</strong> Uses two separate HSV ranges to capture different red shades.</li>
                            <li><strong>Blue Lane:</strong> Detected using a tight HSV range to avoid false positives.</li>
                            <li><strong>Green Lane:</strong> A mask is applied within the green HSV range to isolate the green lane.</li>
                        </ul>
                        
                        <h2>Estimating Distance to the Lane</h2>
                        <p>
                            The detected lane width in pixels is used to estimate the real-world distance. The pinhole camera model formula is applied:
                        </p>
                        <pre>
                    distance_m = (known_lane_width_m * focal_length_px) / detected_pixel_width
                        </pre>
                        <p>
                            This ensures accurate movement decisions based on how far the lane is from the robot.
                        </p>
                        
                        <h2>LED Signaling</h2>
                        <p>
                            The robot uses LEDs to signal turning intentions:
                        </p>
                        <ul>
                            <li>For a <strong>right turn (blue lane)</strong>, the right-side LEDs (front & back) turn red.</li>
                            <li>For a <strong>left turn (green lane)</strong>, the left-side LEDs (front & back) turn red.</li>
                            <li>For a <strong>red lane stop</strong>, no LED signaling is needed.</li>
                        </ul>
                        <p>The LED colors are controlled through ROS by publishing a pattern to the LED topic.</p>
                        
                        <p>
                            The Duckiebot uses LED indicators to signal turning behavior based on detected lanes. The LED system consists of 
                            five individual lights positioned around the robot, each assigned a specific index:
                        </p>
                        <ul>
                            <li><strong>Index 0:</strong> Front-left LED</li>
                            <li><strong>Index 1:</strong> Back-right LED</li>
                            <li><strong>Index 2:</strong> Center LED (not used for signaling)</li>
                            <li><strong>Index 3:</strong> Back-left LED</li>
                            <li><strong>Index 4:</strong> Front-right LED</li>
                        </ul>

                        <h3>Right Turn Signal (Blue Lane)</h3>
                        <p>
                            When the robot detects a blue lane, it activates the <strong>front-right (index 4) and back-right (index 1) LEDs</strong> to signal a right turn. 
                            This is achieved by publishing a pattern where only these indices are set to active while the others remain off.
                        </p>
                        <p>
                            The LED activation pattern for a right turn is:
                        </p>
                        <pre>
                        [0,1,0,0,1]  <!-- Only back-right and front-right LEDs are ON -->
                        </pre>
                        <p>
                            This pattern is sent to the Duckiebot's LED topic:
                        </p>
                        <pre>
                        /csc22907/led_emitter_node/led_pattern
                        </pre>

                        <h3>Left Turn Signal (Green Lane)</h3>
                        <p>
                            When the robot detects a green lane, it activates the <strong>front-left (index 0) and back-left (index 3) LEDs</strong> to signal a left turn. 
                            The activation pattern ensures only the required LEDs are turned on.
                        </p>

                        
                        <h2>Executing Movement</h2>
                        <p>
                            The Duckiebot executes specific behaviors based on the detected lane color:
                        </p>
                        <ul>
                            <li><strong>Blue Lane:</strong> The robot stops, signals a right turn, then performs a 90-degree right curve.</li>
                            <p>
                                When the duckiebot detects blue lane, it calculates how much distance to move and that distance is sent to a straight moving function to move upto that distance. It stops after moving for that distance and does the right turn.
                            </p>
                            <li><strong>Red Lane:</strong> After detecting the red, calculates the distance, move upto the point, then robot stops for 3 seconds and moves forward by 30 cm.</li>

                            <li><strong>Green Lane:</strong> Moves upto the point, stops, signals a left turn, then executes a 90-degree left curve.</li>
                        </ul>
                        <p>
                            The movement is executed using wheel encoder data to ensure accurate positioning.
                        </p>
                        
                        <h2>Execution Log</h2>
                        <pre>
                    [INFO] Lane detected: blue
                    [INFO] Stopping for 3 seconds.
                    [INFO] Signaling right turn (right LEDs red).
                    [INFO] Executing 90-degree right curve.
                    [INFO] Lane detected: red
                    [INFO] Stopping for 3 seconds.
                    [INFO] Moving forward 30 cm.
                    [INFO] Lane detected: green
                    [INFO] Stopping for 3 seconds.
                    [INFO] Signaling left turn (left LEDs red).
                    [INFO] Executing 90-degree left curve.
                        </pre>
                    </section>
                </div>
            </li>
        <h1>Part 2: Controllers</h1>
        <p>
            This section explores in depth different types of controllers that enable the bot to drive
            autonomously. There are three types of controllers we are going to use in this section:
            <ul>
                <li><b>Proportional (P) Controller:</b> Adjusts the control output proportionally to the current error. It’s simple but can lead to steady-state error and overshoot.</li>
                <li><b>Proportional-Derivative (PD) Controller:</b> Combines proportional control with derivative action, helping predict future errors and reducing overshoot and oscillations, resulting in a more stable response.</li>
                <li><b>Proportional-Integral-Derivative (PID) Controller:</b> Adds integral action to PD, eliminating steady-state errors by considering past errors. PID controllers offer robust performance, widely used in autonomous systems for smooth and precise control.</li>
            </ul>
        </p>
        <p>
            In order to implement any of the above controllers (P, PD, PID), the Duckiebot needs to accurately detect two critical lane markings:
            <ul>
                <li><b>Yellow dotted lane:</b> Separates inbound and outbound traffic.</li>
                <li><b>White solid lane:</b> Defines the outer boundary of the lane.</li>
            </ul>

        
<!-- 1. Converting ROS Image to OpenCV and Cropping -->
<h2>1. Converting ROS Image to OpenCV and Cropping</h2>
<p>
   We first convert the incoming <code>CompressedImage</code> from ROS into an OpenCV BGR image.
   Then, we crop the bottom half (where lanes are usually visible) to reduce processing overhead.
</p>
<pre><code>
cv_image = self._bridge.compressed_imgmsg_to_cv2(msg, desired_encoding="bgr8")<br>
height, width, _ = cv_image.shape<br>
bottom_half = cv_image[height // 2:, :]<br>
</code></pre>


<!-- 2. Undistorting the Cropped Image -->
<h2>2. Undistorting the Cropped Image</h2>
<p>
   Using camera calibration parameters (camera matrix &amp; distortion coefficients), we undistort 
   the cropped image so that straight lines in the scene appear straight in the image.
</p>
<pre><code>self.camera_matrix = np.array([
    [263.6565,   0.0,     333.3401],
    [  0.0,    265.4119, 210.1412],
    [  0.0,      0.0,       1.0 ]
])
self.dist_coeffs = np.array([-0.2147, 0.03395, 0.008495, 0.0004646, 0.0])

def undistort_image(self, image):
    new_cam_mtx, roi = cv2.getOptimalNewCameraMatrix(
        self.camera_matrix, 
        self.dist_coeffs, 
        (image.shape[1], image.shape[0]), 
        1, 
        (image.shape[1], image.shape[0])
    )
    undistorted = cv2.undistort(image, self.camera_matrix, self.dist_coeffs, None, new_cam_mtx)
    return cv2.resize(undistorted, (320, 240))
</code></pre>

<!-- 3. Detecting the Yellow Dotted Lane -->
<h2>3. Detecting the Yellow Dotted Lane</h2>
<p>
   We convert the BGR image to HSV and define a range that isolates yellow hues. 
   A binary mask is created using <code>cv2.inRange</code> for the specified HSV thresholds.
</p>
<pre><code>hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
lower_yellow = np.array([20, 75, 100])
upper_yellow = np.array([30, 255, 255])
yellow_mask  = cv2.inRange(hsv, lower_yellow, upper_yellow)
</code></pre>
<ul>
    <li><strong>BGR to HSV:</strong> Easier color segmentation under varying lighting.</li>
    <li><strong>HSV Range (Yellow):</strong> Hue ~20-30, Saturation ~75-255, Value ~100-255.</li>
    <li><strong>Binary Mask:</strong> Highlights yellow lane markings.</li>
</ul>

<!-- 4. Detecting the White Solid Lane -->
<h2>4. Detecting the White Solid Lane</h2>
<p>
   Using the same HSV approach, we define a low-saturation, high-value range to capture white hues.
</p>
<pre><code>hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
lower_white = np.array([0, 0, 135])
upper_white = np.array([180, 29, 255])
white_mask  = cv2.inRange(hsv, lower_white, upper_white)
</code></pre>
<ul>
    <li><strong>HSV Range (White):</strong> Low saturation (0-29), high brightness (135+).</li>
    <li><strong>Binary Mask:</strong> Highlights white lane boundaries.</li>
</ul>

<!-- 5. Combining Masks and Publishing to ROS Topic -->
<h2>5. Combining Masks and Publishing to ROS Topic</h2>
<pre><code>combined_mask = cv2.bitwise_or(yellow_mask, white_mask)

self.pub_both.publish(
    self._bridge.cv2_to_compressed_imgmsg(combined_mask)
)
</code></pre>
<ul>
    <li><strong>Merging Lane Masks:</strong> Combines both yellow and white detections.</li>
    <li><strong>ROS Publisher:</strong> Publishes the resulting mask for further use.</li>
</ul>



        

   <h2>Using Masks to Compute Lane Center for Autonomous Control</h2>

<p>
    After detecting lanes (yellow dotted and white solid) and generating binary masks, computing the lane center is essential for autonomous navigation.
</p>

<h3>1. Computing Lane Center from Masks</h3>
<ul>
    <li>Use contours from binary masks to identify lane boundaries.</li>
    <li>Calculate centroids of these contours to estimate lane positions.</li>
    <li>Determine the lane center based on available centroids.</li>
</ul>

<h3>Important Python Functions:</h3>
<pre><code>
# Find contours
contours_white, _ = cv2.findContours(white_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
contours_yellow, _ = cv2.findContours(yellow_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# Find centroid (example snippet)
def find_centroid(self, contours):
    largest_contour = max(contours, key=cv2.contourArea)
    moments = cv2.moments(largest_contour)

    if moments['m00'] != 0:
        cx = int(moments['m10'] / moments['m00'])
        cy = int(moments['m01'] / moments['m00'])
        return (cx, cy)
    return None
</code></pre>

<h3>2. Explanation of the Approach</h3>
<ul>
    <li><b>Contours:</b> Identified boundaries in binary lane masks.</li>
    <li><b>Centroids:</b> Position estimates of lane boundaries.</li>
    <li><b>Lane Center:</b> Average or select centroids depending on visibility:
        <ul>
            <li>If both lanes detected, average their centroids.</li>
            <li>If one lane is missing, use the detected lane centroid.</li>
        </ul>
    </li>
</ul>

<h3>3. Integrating Lane Center with Controllers</h3>
<p>
The computed lane center is compared to the image's horizontal midpoint. The difference provides an error signal used by steering controllers (P, PD, PID):
</p>

<h3>Integrating Lane Center with Controllers</h3>
<p>
The computed lane center is compared to the image center. The difference (error) is used by steering controllers.
</p>

<!-- (a) Pros and Cons of P, PD, and PID Controllers -->
<h3>a) What are the pros and cons of “P,” “PD,” and “PID” controllers?</h3>

<h4>P Controller Logic:</h4>
<pre><code>def apply_p_controller(self, lane_center, image):
    image_center = image.shape[1] // 2
    error = image_center - lane_center
    control = self.Kp * error
    base_speed = 0.3
    vel_left = base_speed + control
    vel_right = base_speed - control
</code></pre>
<ul>
    <li><b>Pros:</b> Simple to implement and provides a relatively fast response.</li>
    <li><b>Cons:</b> Tends to leave a steady-state error (offset) and may cause overshoot if Kp is large.</li>
</ul>

<h4>PD Controller Logic:</h4>
<pre><code>def apply_pd_controller(self, lane_center, image):
    image_center = image.shape[1] // 2
    error = image_center - lane_center
    derivative = error - self.prev_error
    self.prev_error = error
    control = self.Kp * error + self.Kd * derivative
    base_speed = 0.4
    vel_left = base_speed + control
    vel_right = base_speed - control
</code></pre>
<ul>
    <li><b>Pros:</b> The derivative term helps reduce overshoot and allows the system to stabilize faster.</li>
    <li><b>Cons:</b> More sensitive to noise. If the derivative gain is too high, it can amplify small measurement errors.</li>
</ul>

<h4>PID Controller Logic (with Anti-Windup):</h4>
<pre><code>def apply_pid_controller(self, lane_center, image):
    image_center = image.shape[1] // 2
    error = lane_center - image_center
    derivative = error - self.prev_error
    self.prev_error = error
    self.integral += error
    
    # Anti-windup to prevent integral saturation
    self.integral = max(min(self.integral, self.max_integral), -self.max_integral)

    control = self.Kp * error + self.Ki * self.integral + self.Kd * derivative
    base_speed = 0.25
    vel_left = max(min(base_speed + control, 1.0), -1.0)
    vel_right = max(min(base_speed - control, 1.0), -1.0)

    cmd = WheelsCmdStamped()
    cmd.vel_left = vel_left
    cmd.vel_right = vel_right
    self.publisher.publish(cmd)
</code></pre>
<ul>
    <li><b>Pros:</b> Eliminates steady-state error through the integral term and provides a stable response under diverse conditions.</li>
    <li><b>Cons:</b> Tuning is more complex. Without anti-windup, the integral term can grow excessively (windup).</li>
</ul>

<!-- (b) The Error Calculation Method -->
<h3>b) What is the error calculation method for your controller?</h3>
<p>
    We compute the difference between the image center (i.e., the midpoint of the camera’s horizontal field) 
    and the detected lane center. For example:
</p>
<pre><code>error = image_center - lane_center
</code></pre>
<p>
    This <code>error</code> tells us how far off-center we are. 
    A positive error indicates the lane is to one side, and a negative error indicates the other side.
</p>

<!-- (c) Derivative Term Impact -->
<h3>c) How does the derivative (“D”) term in the “PD” controller impact control logic? Is it beneficial?</h3>
<p>
    The derivative term (<code>Kd * derivative</code>) responds to the rate of change of the error. 
    It helps reduce overshoot and smooth out the response by reacting to rapidly changing errors. 
    This can be beneficial if properly tuned; however, it also makes the controller more sensitive to noise, 
    which can introduce jitter or instability if <code>Kd</code> is set too high.
</p>

<!-- (d) Integral Term Impact -->
<h3>d) How does the integral (“I”) term in the “PID” controller affect performance? Was it useful for your robot?</h3>
<p>
    The integral term (<code>Ki * integral</code>) accumulates the error over time to eliminate steady-state errors. 
    This helps the robot remain centered in the lane, even if there are persistent biases or slight slopes in the environment. 
    However, if <code>Ki</code> is not tuned correctly or if anti-windup is not implemented, the integral can grow 
    too large and cause significant oscillations. In many lane-following scenarios, a small integral gain can be very useful 
    to counteract slight misalignments or biases.
</p>

<!-- (e) Methods to Tune the Controller's Parameters -->
<h3>e) What methods can be used to tune the controller’s parameters effectively?</h3>
<ul>
    <li><b>Manual/Iterative Tuning:</b> Gradually adjust <code>Kp</code>, <code>Ki</code>, and <code>Kd</code> by observing the robot's behavior (overshoot, steady-state error, oscillations) and refine them step by step.</li>
    <li><b>Ziegler–Nichols Method:</b> A classical approach using the critical gain and oscillation period to set <code>P</code>, <code>I</code>, and <code>D</code> values.</li>
    <li><b>Automated Tuning:</b> Advanced techniques or machine-learning-based methods can systematically search for the optimal parameters.</li>
    <li><b>Simulation Before Real Deployment:</b> Use a simulator to test and tune gains in a safe environment.</li>
</ul>


        <!-- Additional Explanation on Actual Tuning Values -->
<h3>Additional Notes on Tuning Values</h3>
<p>
    In the code, we used:
</p>
<pre><code>self.Kp = 0.0013
self.Kd = 0.005
self.Ki = 0.001
self.max_integral = 20  # For anti-windup
</code></pre>
<p>
    These values were primarily found through trial and error. We started from a baseline (like <code>0.05</code>) and then:
</p>
<ul>
    <li><b>Reduced <code>Kp</code></b> until the oscillations were minimized but the robot still corrected its course in time.</li>
    <li><b>Adjusted <code>Kd</code></b> to dampen overshoot. Higher <code>Kd</code> provides quicker damping but can amplify noise.</li>
    <li><b>Introduced <code>Ki</code></b> in small increments to remove any steady-state error. 
        Once the robot could maintain a centered position, <code>Ki</code> was deemed sufficient.</li>
    <li><b>Set <code>max_integral</code></b> to <code>20</code> to cap the integral growth and prevent windup issues.</li>
</ul>
        


        <li>
            <h3>A video of the Duckiebot going straight P controller</h3>
            <video width="auto" height="600" controls>
                <source src="videos/P_S.mov" type="video/mp4">
                Your 
            </video>
        </li>
        <li>                
            <h3>A video of the Duckiebot going straight PI controller</h3>
            <video width="auto" height="600" controls>
                <source src="videos/PI_S.mov" type="video/mp4">
                Your 
            </video>
        </li>
        <li>
            <h3>A video of the Duckiebot going straight PID controller</h3>
            <video width="auto" height="600" controls>
                <source src="videos/PID_s.mov" type="video/mp4">
                Your 
            </video>
        </li>
        <li>
            <h3>A video of the Duckiebot doing lane-following</h3>
            <video width="auto" height="600" controls>
                <source src="videos/our_lane_follow.mov" type="video/mp4">
                Your 
            </video>
        </li>


    </section>
                
    <section class="references">
        <h2>References</h2>
        <ul>
            <li>ChatGPT was used for help with making the website and understanding some topics like P, PD and PID controllers</li>
            <li> 
                <a href="https://samuelfneumann.github.io/posts/duckie_4/">Duckietown Blog - Lane Detection</a>
            </li>
            
        </ul>
    </section>
    <h3>By: Sandhya Adhikari and Fumanpreet Singh</h1>
    
    <a href="index.html">Back to Assignments</a>
</body>
</html>
 
