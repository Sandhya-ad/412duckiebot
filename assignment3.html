<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assignment 3</title>
    <link rel="stylesheet" href="templates/assignment3.css">
</head>
<body>
    <h1>Assignment 3 </h1>
    <section class="deliverables">
        <h1>Part 1: Computer Vision</h1>
        <ul>
            <li>
                A screen capture of the distorted camera image
                <div class="image-container">
                    <img src="images/distorted.png" alt="Distorted image" class="deliverable-photo">
                </div>
                <div class="report">
                    <h2>Camera Distortion</h2>
                    <p>
                        Camera distortion occurs due to the curvature of the lens, which bends light unevenly as it enters the camera sensor. This effect is particularly noticeable in radial distortion, where straight lines appear curved, especially near the edges of the image. We can see the same thing happen to our camera image in the above picture, especially near the edges, lines appear bent
                        </p>
                    <p>
                        <strong>Subscribing to camera topic: </strong>
                        To capture a distorted image, we subscribe to the ROS topic /camera_node/image/compressed, which provides a compressed stream of images from the camera. The subscription allows us to receive and process real-time camera data in ROS.
                    </p>
                </div>
            </li>

            <li>
                A screen capture of the undistorted camera image
                <div class="image-container">
                    <img src="images/undistorted_blur.png" alt="Distorted image" class="deliverable-photo">
                </div>
                <div class="report">
                    <h2>Undistorting the Image</h2>
                    <p>
                        To correct lens distortion, we use <strong> intrinsic parameters</strong> that describe the camera’s internal properties, such as focal length, optical center, and distortion coefficients. These parameters allow us to mathematically reverse the distortion effect and obtain a properly aligned image.
                    </p>
                    <p>
                        By performing these steps, we ensure that the camera feed provides an accurate and geometrically correct 
                        representation of the environment, which is essential for tasks such as color detection and lane following.
                    </p>
                    
                    <h3>What we did</h3>
                    
                    <ul>
                        <li>
                            <strong>Subscribing to the camera topic:</strong>
                            The robot receives real-time images from the ROS topic <code>/camera_node/image/compressed</code>. 
                            These images contain lens distortions that need correction before they can be used for accurate perception.
                        </li>
                        
                        <li>
                            <strong>Retrieving camera intrinsic parameters:</strong>
                            The robot's camera has unique calibration parameters stored in <code>/camera_node/camera_info</code>. 
                            These include:
                            <ul>
                                <li><strong>Camera Matrix:</strong> Defines how the camera maps the 3D world to a 2D image.</li>
                                <li><strong>Distortion Coefficients:</strong> Corrects radial and tangential distortions.</li>
                            </ul>
                            Using these parameters ensures that undistortion is tailored specifically for the robot's camera.
                        </li>
                        
                        <li>
                            <strong>Calculating an optimal new camera matrix:</strong>
                            Using <code>cv2.getOptimalNewCameraMatrix()</code>, we refine the camera matrix to minimize unwanted black borders 
                            in the corrected image. This step improves the visual quality of the output.
                        </li>
                    
                        <li>
                            <strong>Applying image correction:</strong>
                            The <code>cv2.undistort()</code> function is used to transform the distorted image into an undistorted one. 
                            This ensures that straight lines appear as they should, and objects are accurately represented.
                        </li>
                    
                        <li>
                            <strong>Publishing the corrected image:</strong>
                            The final undistorted image is then published to <code>/camera_undistorted/image/compressed</code>. 
                            This allows other ROS nodes, such as those responsible for <strong>color detection and lane following</strong>, to work 
                            with a geometrically correct image.
                        </li>
                    </ul>
                    
                    <h3>Purpose:</h3>

                    <p>
                        Undistortion is essential for accurate lane detection and navigation. When an image is distorted, straight lane markings can appear curved, leading to incorrect edge detection and misalignment. By correcting distortion, the robot can accurately perceive lane boundaries and navigate accordingly. Additionally, distorted images can stretch or compress objects, causing inconsistencies in object sizing. Undistortion ensures that objects appear with correct proportions, improving the accuracy of both lane detection and obstacle recognition. Color detection also benefits from undistortion, as color inconsistencies often occur near the edges of distorted images. By stabilizing the image, the robot can reliably detect lane colors without interference from warped color transitions. Finally, distance estimation depends on an accurate visual representation of the environment. Since distorted images alter the perceived shape and position of objects, correcting the distortion ensures that distance calculations remain precise, leading to more reliable navigation and decision-making.
                    </p>
                    
                    <ul>
                        <li>
                            <h2>Lane Detection Contours</h2>
                            <p>
                                The images below show the detected contours for <strong>blue, red, and green lanes</strong> using 
                                our ROS subscriber in <code>rqt_image_view</code>. The contours are drawn based on 
                                <strong>color segmentation and edge detection</strong>, ensuring accurate lane identification.
                            </p>
            
                            <h3>Contours Detecting a Blue Line</h3>
                            <div class="image-container">
                                <img src="images/blue_detection.png" alt="Blue Lane Contours" class="deliverable-photo">
                            </div>
                            <p>
                                The above image shows how the <strong>blue lane</strong> is detected. The contours are drawn 
                                around the largest detected blue area using <strong>HSV filtering and contour extraction</strong>.
                            </p>
            
                            <h3>Contours Detecting a Red Line</h3>
                            <div class="image-container">
                                <img src="images/red_detection.png" alt="Red Lane Contours" class="deliverable-photo">
                            </div>
                            <p>
                                This image demonstrates the detection of the <strong>red lane</strong>. Since red appears in two 
                                sections of the <strong>HSV color space</strong>, we use two separate hue ranges to ensure complete 
                                detection. The detected lane is highlighted with contours.
                            </p>
            
                            <h3>Contours Detecting a Green Line</h3>
                            <div class="image-container">
                                <img src="images/green_detection.png" alt="Green Lane Contours" class="deliverable-photo">
                            </div>
                            <p>
                                The <strong>green lane</strong> is detected by applying a mask within the green HSV range. The 
                                contours drawn on the detected area highlight the extracted lane.
                            </p>
                        </li>
            
                        <li>
                            <h2>Explanation of Color Detection</h2>

                            <h3>Preprocessing the Image</h3>
                            <p>
                                The camera feed is first undistorted to correct for lens distortion. 
                                Then, the image is converted from </strong>BGR to HSV format</strong> to separate 
                                color components, making color segmentation more reliable.
                            </p>
                            <p>
                                Color detection is performed by converting the <strong>BGR image to HSV format</strong> and 
                                applying predefined HSV thresholds. The <strong>HSV (Hue, Saturation, Value) color model</strong> 
                                is preferred because it separates color intensity from brightness, making it more 
                                reliable under different lighting conditions.
                            </p>
            
                            <h2>How Color Detection Works</h2>
                            <h3> Applying Color Thresholding</h3>
                            <p>
                                The HSV model is used to filter out specific colors corresponding to 
                                <strong>red, blue, and green lanes</strong>. The following ranges are applied:
                            </p>
                            <ul>
                                <li><strong>Red Lane:</strong> <code>Lower: [0, 100, 100] & [170, 100, 100], Upper: [10, 255, 255] & [180, 255, 255]</code></li>
                                <li><strong>Blue Lane:</strong> <code>Lower: [100, 150, 100], Upper: [130, 255, 255]</code></li>
                                <li><strong>Green Lane:</strong> <code>Lower: [46, 50, 65], Upper: [95, 196, 199]</code></li>
                            </ul>
                            <p>
                                Each color mask isolates only the pixels matching the specified HSV range, 
                                filtering out everything else.
                            </p>
            
                            <h3>3. Removing Noise</h3>
                            <p>
                                Morphological operations, such as </strong>closing</strong>, are applied to remove 
                                small artifacts that could cause false detections.
                            </p>
            
                            <h3>4. Finding and Drawing Contours</h3>
                            <p>
                                Using <code>cv2.findContours()</code>, lane boundaries are detected 
                                and enclosed in a bounding box using <code>cv2.boundingRect()</code>. 
                                The largest detected contour for each color is selected.
                            </p>
            
                            <h3>5. Estimating Distance to the Lane</h3>
                            <p>
                                The width of the detected lane in pixels is used to estimate its 
                                real-world distance using the <strong>pinhole camera model</strong>:
                            </p>
                            <pre>
    distance_m = (known_object_width_m * focal_length_px) / object_pixel_width
                            </pre>
                            <p>
                                The known width of the lane is a predefined constant, and the 
                                focal length is retrieved from the camera’s intrinsic parameters.
                                This is used to see how far the object is from the robot
                            </p>
            
                            <h3>6. Publishing the Detected Lane</h3>
                            <p>
                                The processed image, with detected lanes and distance labels, 
                                is published to <code>/lane_detection/image/compressed</code> for 
                                integration with navigation modules.
                                The dimensions of the detected lane is also published in the terminal.
                                Example of our output:
                                <pre>
    [INFO] [1740622024.705454]: Published lane detection image.
    [INFO] [1740622024.751194]: Detected: red, Dimensions: (240, 23), Distance: 0.223m
    [INFO] [1740622024.772927]: Published lane detection image.
    [INFO] [1740622024.830412]: Detected: red, Dimensions: (242, 22), Distance: 0.221m
                                </pre>

                            </p>
                        </li>
            
                        <li>
                            <h2>Summary of Detection for Each Color</h2>
            
                            <h3>Red Lane Detection</h3>
                            <p>
                                - <strong>Uses two HSV ranges</strong> to capture different red shades.
                                - Filters out yellow regions that may overlap with red.
                                - Extracts the <strong>largest contour</strong> and highlights it.
                                - <strong>Used to detect stop points</strong> and straight movement areas.
                            </p>
            
                            <h3>Blue Lane Detection</h3>
                            <p>
                                - Detects blue lanes using a <strong>tight HSV range</strong>.
                                - Filters out shadows and dark patches.
                                - Contours are enclosed in a bounding box.
                                - <strong>Used to trigger right turns</strong> in the navigation system.
                            </p>
            
                            <h3>Green Lane Detection</h3>
                            <p>
                                - Filters out unwanted environmental colors.
                                - Detects <strong>left-turn trigger points</strong>.
                                - Bounding boxes are drawn around the detected lane.
                            </p>
                        </li>
                    </ul>
                        </li>
            
                        <li>
                            <h2>How to Tune HSV Parameters</h2>
                            <p>
                                The HSV values for each lane color are adjusted to optimize detection. The tuning process involves:
                            </p>
                            <ul>
                                <li>
                                    <strong>Increasing or decreasing the Hue (H) range</strong> to fine-tune color recognition. 
                                    If a color is not detected properly, the hue range is <strong>expanded or shifted</strong>.
                                </li>
                                <li>
                                    <strong>Adjusting the Saturation (S) range</strong> to control how vivid the colors need to be. 
                                    This helps filter out <strong>faded colors</strong> and background noise.
                                </li>
                                <li>
                                    <strong>Modifying the Value (V) range</strong> to adjust brightness sensitivity. Raising the 
                                    lower limit reduces <strong>false detections in dark regions</strong>.
                                </li>
                                <li>
                                    <strong>Testing under different lighting conditions</strong> to ensure the values work consistently.
                                </li>
                            </ul>
                            <p>
                                The final <strong>HSV thresholds</strong> used for lane detection are:
                            </p>
                            <ul>
                                <li><strong>Blue Lane:</strong> <code>Lower: [100, 150, 100], Upper: [130, 255, 255]</code></li>
                                <li><strong>Red Lane:</strong> <code>Lower: [0, 100, 100] & [170, 100, 100], Upper: [10, 255, 255] & [180, 255, 255]</code></li>
                                <li><strong>Green Lane:</strong> <code>Lower: [46, 50, 65], Upper: [95, 196, 199]</code></li>
                            </ul>
                        </li>
                    </ul>
                    <ul>
                        <li>
                            <h3>A video of the Duckiebot going straight until red line stopping and moving 30 cm ahead</h3>
                            <video width="auto" height="600" controls>
                                <source src="videos/red_lane.mp4" type="video/mp4">
                                Your 
                            </video>
                        </li>
                        <li>
                            <h3>A video of the Duckiebot going straight until blue line stopping then turning right after turning the right lights to red</h3>
                            <video width="auto" height="600" controls>
                                <source src="videos/blue_lane.mp4" type="video/mp4">
                                Your 
                            </video>
                        </li>
                        <li>
                            <h3>A video of the Duckiebot going straight until green line stopping then turning left after turning the left lights to red</h3>
                            <video width="auto" height="600" controls>
                                <source src="videos/green_lane.mp4" type="video/mp4">
                                Your 
                            </video>
                        </li>
                    </ul>
                    <h1>Lane Behavior Execution Report</h1>
                    <section class="deliverables">
                        
                        <h2>Detecting Lane Colors</h2>
                        <p>
                            The Duckiebot detects lanes using a camera feed, which is processed in real time. The image is converted from BGR to HSV format, allowing the system to separate color information efficiently. Specific HSV thresholds are applied to filter out the target colors:
                        </p>
                        <ul>
                            <li><strong>Red Lane:</strong> Uses two separate HSV ranges to capture different red shades.</li>
                            <li><strong>Blue Lane:</strong> Detected using a tight HSV range to avoid false positives.</li>
                            <li><strong>Green Lane:</strong> A mask is applied within the green HSV range to isolate the green lane.</li>
                        </ul>
                        
                        <h2>Estimating Distance to the Lane</h2>
                        <p>
                            The detected lane width in pixels is used to estimate the real-world distance. The pinhole camera model formula is applied:
                        </p>
                        <pre>
                    distance_m = (known_lane_width_m * focal_length_px) / detected_pixel_width
                        </pre>
                                      <pre>
                  known_lane_width_m is the width of the tape on the road
                        </pre>
                        <p>
                            This ensures accurate movement decisions based on how far the lane is from the robot.
                        </p>
                        
                        <h2>LED Signaling</h2>
                        <p>
                            The robot uses LEDs to signal turning intentions:
                        </p>
                        <ul>
                            <li>For a <strong>right turn (blue lane)</strong>, the right-side LEDs (front & back) turn red.</li>
                            <li>For a <strong>left turn (green lane)</strong>, the left-side LEDs (front & back) turn red.</li>
                            <li>For a <strong>red lane stop</strong>, no LED signaling is needed.</li>
                        </ul>
                        <p>The LED colors are controlled through ROS by publishing a pattern to the LED topic.</p>
                        
                        <p>
                            The Duckiebot uses LED indicators to signal turning behavior based on detected lanes. The LED system consists of 
                            five individual lights positioned around the robot, each assigned a specific index:
                        </p>
                        <ul>
                            <li><strong>Index 0:</strong> Front-left LED</li>
                            <li><strong>Index 1:</strong> Back-right LED</li>
                            <li><strong>Index 2:</strong> Center LED (not used for signaling)</li>
                            <li><strong>Index 3:</strong> Back-left LED</li>
                            <li><strong>Index 4:</strong> Front-right LED</li>
                        </ul>

                        <h3>Right Turn Signal (Blue Lane)</h3>
                        <p>
                            When the robot detects a blue lane, it activates the <strong>front-right (index 4) and back-right (index 1) LEDs</strong> to signal a right turn. 
                            This is achieved by publishing a pattern where only these indices are set to active while the others remain off.
                        </p>
                        <p>
                            The LED activation pattern for a right turn is:
                        </p>
                        <pre>
                        [0,1,0,0,1]  <!-- Only back-right and front-right LEDs are ON -->
                        </pre>
                        <p>
                            This pattern is sent to the Duckiebot's LED topic:
                        </p>
                        <pre>
                        /csc22907/led_emitter_node/led_pattern
                        </pre>

                        <h3>Left Turn Signal (Green Lane)</h3>
                        <p>
                            When the robot detects a green lane, it activates the <strong>front-left (index 0) and back-left (index 3) LEDs</strong> to signal a left turn. 
                            The activation pattern ensures only the required LEDs are turned on.
                        </p>

                        
                        <h2>Executing Movement</h2>
                        <p>
                            The Duckiebot executes specific behaviors based on the detected lane color:
                        </p>
                        <ul>
                            <li><strong>Blue Lane:</strong> The robot stops, signals a right turn, then performs a 90-degree right curve.</li>
                            <p>
                                When the duckiebot detects blue lane, it calculates how much distance to move and that distance is sent to a straight moving function to move upto that distance. It stops after moving for that distance and does the right turn.
                            </p>
                            <li><strong>Red Lane:</strong> After detecting the red, calculates the distance, move upto the point, then robot stops for 3 seconds and moves forward by 30 cm.</li>

                            <li><strong>Green Lane:</strong> Moves upto the point, stops, signals a left turn, then executes a 90-degree left curve.</li>
                        </ul>
                        <p>
                            The movement is executed using wheel encoder data to ensure accurate positioning.
                        </p>
                        
                        <h2>Execution Log</h2>
                        <pre>
                    [INFO] Lane detected: blue
                    [INFO] Stopping for 3 seconds.
                    [INFO] Signaling right turn (right LEDs red).
                    [INFO] Executing 90-degree right curve.
                    [INFO] Lane detected: red
                    [INFO] Stopping for 3 seconds.
                    [INFO] Moving forward 30 cm.
                    [INFO] Lane detected: green
                    [INFO] Stopping for 3 seconds.
                    [INFO] Signaling left turn (left LEDs red).
                    [INFO] Executing 90-degree left curve.
                        </pre>
                    </section>
                </div>
            </li>
        <h1>Part 2: Controllers</h1>
        <p>
            This section explores in depth different types of controllers that enable the bot to drive
            autonomously. There are three types of controllers we are going to use in this section:
            <ul>
                <li><b>Proportional (P) Controller:</b> Adjusts the control output proportionally to the current error. It’s simple but can lead to steady-state error and overshoot.</li>
                <li><b>Proportional-Derivative (PD) Controller:</b> Combines proportional control with derivative action, helping predict future errors and reducing overshoot and oscillations, resulting in a more stable response.</li>
                <li><b>Proportional-Integral-Derivative (PID) Controller:</b> Adds integral action to PD, eliminating steady-state errors by considering past errors. PID controllers offer robust performance, widely used in autonomous systems for smooth and precise control.</li>
            </ul>
        </p>
        <p>
            In order to implement any of the above controllers (P, PD, PID), the Duckiebot needs to accurately detect two critical lane markings:
            <ul>
                <li><b>Yellow dotted lane:</b> Separates inbound and outbound traffic.</li>
                <li><b>White solid lane:</b> Defines the outer boundary of the lane.</li>
            </ul>

        
<!-- 1. Converting ROS Image to OpenCV and Cropping -->
<h2>1. Converting ROS Image to OpenCV and Cropping</h2>
<p>
   We first convert the incoming <code>CompressedImage</code> from ROS into an OpenCV BGR image.
   Then, we crop the bottom half (where lanes are usually visible) to reduce processing overhead.
</p>
<pre><code>
cv_image = self._bridge.compressed_imgmsg_to_cv2(msg, desired_encoding="bgr8")<br>
height, width, _ = cv_image.shape<br>
bottom_half = cv_image[height // 2:, :]<br>
</code></pre>


<!-- 2. Undistorting the Cropped Image -->
<h2>2. Undistorting the Cropped Image</h2>
<p>
   Using camera calibration parameters (camera matrix &amp; distortion coefficients), we undistort 
   the cropped image so that straight lines in the scene appear straight in the image.
</p>
<pre><code>self.camera_matrix = np.array([
    [263.6565,   0.0,     333.3401],
    [  0.0,    265.4119, 210.1412],
    [  0.0,      0.0,       1.0 ]
])<br>
self.dist_coeffs = np.array([-0.2147, 0.03395, 0.008495, 0.0004646, 0.0])<br>

def undistort_image(self, image):<br>
    new_cam_mtx, roi = cv2.getOptimalNewCameraMatrix(
        self.camera_matrix, 
        self.dist_coeffs, 
        (image.shape[1], image.shape[0]), 
        1, 
        (image.shape[1], image.shape[0])
    )<br>
    undistorted = cv2.undistort(image, self.camera_matrix, self.dist_coeffs, None, new_cam_mtx)<br>
    return cv2.resize(undistorted, (320, 240))<br>
</code></pre>

<!-- 3. Detecting the Yellow Dotted Lane -->
<h2>3. Detecting the Yellow Dotted Lane</h2>
<p>
   We convert the BGR image to HSV and define a range that isolates yellow hues. 
   A binary mask is created using <code>cv2.inRange</code> for the specified HSV thresholds.
</p>
<pre><code>hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)<br>
lower_yellow = np.array([20, 75, 100])<br>
upper_yellow = np.array([30, 255, 255])<br>
yellow_mask  = cv2.inRange(hsv, lower_yellow, upper_yellow)<br>
</code></pre>
<ul>
    <li><strong>BGR to HSV:</strong> Easier color segmentation under varying lighting.</li>
    <li><strong>HSV Range (Yellow):</strong> Hue ~20-30, Saturation ~75-255, Value ~100-255.</li>
    <li><strong>Binary Mask:</strong> Highlights yellow lane markings.</li>
</ul>

<!-- 4. Detecting the White Solid Lane -->
<h2>4. Detecting the White Solid Lane</h2>
<p>
   Using the same HSV approach, we define a low-saturation, high-value range to capture white hues.
</p>
<pre><code>hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)<br>
lower_white = np.array([0, 0, 135])<br>
upper_white = np.array([180, 29, 255])<br>
white_mask  = cv2.inRange(hsv, lower_white, upper_white)<br>
</code></pre>
<ul>
    <li><strong>HSV Range (White):</strong> Low saturation (0-29), high brightness (135+).</li>
    <li><strong>Binary Mask:</strong> Highlights white lane boundaries.</li>
</ul>

<!-- 5. Combining Masks and Publishing to ROS Topic -->
<h2>5. Combining Masks and Publishing to ROS Topic</h2>
<pre><code>combined_mask = cv2.bitwise_or(yellow_mask, white_mask)<br>

self.pub_both.publish(
    self._bridge.cv2_to_compressed_imgmsg(combined_mask)
)<br>
</code></pre>
<ul>
    <li><strong>Merging Lane Masks:</strong> Combines both yellow and white detections.</li>
    <li><strong>ROS Publisher:</strong> Publishes the resulting mask for further use.</li>
</ul>



        

   <h2>Using Masks to Compute Lane Center for Autonomous Control</h2>

<p>
    After detecting lanes (yellow dotted and white solid) and generating binary masks, computing the lane center is essential for autonomous navigation.
</p>

<h3>1. Computing Lane Center from Masks</h3>
<ul>
    <li>Use contours from binary masks to identify lane boundaries.</li>
    <li>Calculate centroids of these contours to estimate lane positions.</li>
    <li>Determine the lane center based on available centroids.</li>
</ul>

<h3>Important Python Functions:</h3>
<pre><code>
# Find contours<br>
contours_white, _ = cv2.findContours(white_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)<br>
contours_yellow, _ = cv2.findContours(yellow_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)<br>
<br>
# Find centroid (example snippet)<br>
def find_centroid(self, contours):<br>
    largest_contour = max(contours, key=cv2.contourArea)<br>
    moments = cv2.moments(largest_contour)<br>
<br>
    if moments['m00'] != 0:<br>
        cx = int(moments['m10'] / moments['m00'])<br>
        cy = int(moments['m01'] / moments['m00'])<br>
        return (cx, cy)<br>
    return None<br>
</code></pre>

<h3>2. Explanation of the Approach</h3>
<ul>
    <li><b>Contours:</b> Identified boundaries in binary lane masks.</li>
    <li><b>Centroids:</b> Position estimates of lane boundaries.</li>
    <li><b>Lane Center:</b> Average or select centroids depending on visibility:
        <ul>
            <li>If both lanes detected, average their centroids.</li>
            <li>If one lane is missing, use the detected lane centroid.</li>
        </ul>
    </li>
</ul>

<h3>3. Integrating Lane Center with Controllers</h3>
<p>
The computed lane center is compared to the image's horizontal midpoint. The difference provides an error signal used by steering controllers (P, PD, PID):
</p>

<h3>Integrating Lane Center with Controllers</h3>
<p>
The computed lane center is compared to the image center. The difference (error) is used by steering controllers.
</p>

<!-- (a) Pros and Cons of P, PD, and PID Controllers -->
<h3>a) What are the pros and cons of “P,” “PD,” and “PID” controllers?</h3>

<h4>P Controller Logic:</h4>
<pre><code>def apply_p_controller(self, lane_center, image):<br>
    image_center = image.shape[1] // 2<br>
    error = image_center - lane_center<br>
    control = self.Kp * error<br>
    base_speed = 0.3<br>
    vel_left = base_speed + control<br>
    vel_right = base_speed - control<br>
</code></pre>
<ul>
    <li><b>Pros:</b> Simple to implement and provides a relatively fast response.</li>
    <li><b>Cons:</b> Tends to leave a steady-state error (offset) and may cause overshoot if Kp is large.</li>
</ul>

<h4>PD Controller Logic:</h4>
<pre><code>def apply_pd_controller(self, lane_center, image):<br>
    image_center = image.shape[1] // 2<br>
    error = image_center - lane_center<br>
    derivative = error - self.prev_error<br>
    self.prev_error = error<br>
    control = self.Kp * error + self.Kd * derivative<br>
    base_speed = 0.4<br>
    vel_left = base_speed + control<br>
    vel_right = base_speed - control<br>
</code></pre>
<ul>
    <li><b>Pros:</b> The derivative term helps reduce overshoot and allows the system to stabilize faster.</li>
    <li><b>Cons:</b> More sensitive to noise. If the derivative gain is too high, it can amplify small measurement errors.</li>
</ul>

<h4>PID Controller Logic (with Anti-Windup):</h4>
<pre><code>def apply_pid_controller(self, lane_center, image):<br>
    image_center = image.shape[1] // 2<br>
    error = lane_center - image_center<br>
    derivative = error - self.prev_error<br>
    self.prev_error = error<br>
    self.integral += error<br>
    <br>
    # Anti-windup to prevent integral saturation<br>
    self.integral = max(min(self.integral, self.max_integral), -self.max_integral)<br>
<br>
    control = self.Kp * error + self.Ki * self.integral + self.Kd * derivative<br>
    base_speed = 0.25<br>
    vel_left = max(min(base_speed + control, 1.0), -1.0)<br>
    vel_right = max(min(base_speed - control, 1.0), -1.0)<br>
    <br>
    cmd = WheelsCmdStamped()<br>
    cmd.vel_left = vel_left<br>
    cmd.vel_right = vel_right<br>
    self.publisher.publish(cmd)<br>
</code></pre>
<ul>
    <li><b>Pros:</b> Eliminates steady-state error through the integral term and provides a stable response under diverse conditions.</li>
    <li><b>Cons:</b> Tuning is more complex. Without anti-windup, the integral term can grow excessively (windup).</li>
</ul>

<!-- (b) The Error Calculation Method -->
<h3>b) What is the error calculation method for your controller?</h3>
<p>
    We compute the difference between the image center (i.e., the midpoint of the camera’s horizontal field) 
    and the detected lane center. For example:
</p>
<pre><code>error = image_center - lane_center
</code></pre>
<p>
    This <code>error</code> tells us how far off-center we are. 
    A positive error indicates the lane is to one side, and a negative error indicates the other side.
</p>

<!-- (c) Derivative Term Impact -->
<h3>c) How does the derivative (“D”) term in the “PD” controller impact control logic? Is it beneficial?</h3>
<p>
    The derivative term (<code>Kd * derivative</code>) responds to the rate of change of the error. 
    It helps reduce overshoot and smooth out the response by reacting to rapidly changing errors. 
    This can be beneficial if properly tuned; however, it also makes the controller more sensitive to noise, 
    which can introduce jitter or instability if <code>Kd</code> is set too high.
</p>

<!-- (d) Integral Term Impact -->
<h3>d) How does the integral (“I”) term in the “PID” controller affect performance? Was it useful for your robot?</h3>
<p>
    The integral term (<code>Ki * integral</code>) accumulates the error over time to eliminate steady-state errors. 
    This helps the robot remain centered in the lane, even if there are persistent biases or slight slopes in the environment. 
    However, if <code>Ki</code> is not tuned correctly or if anti-windup is not implemented, the integral can grow 
    too large and cause significant oscillations. In many lane-following scenarios, a small integral gain can be very useful 
    to counteract slight misalignments or biases.
</p>

<!-- (e) Methods to Tune the Controller's Parameters -->
<h3>e) What methods can be used to tune the controller’s parameters effectively?</h3>
<ul>
    <li><b>Manual/Iterative Tuning:</b> Gradually adjust <code>Kp</code>, <code>Ki</code>, and <code>Kd</code> by observing the robot's behavior (overshoot, steady-state error, oscillations) and refine them step by step.</li>
    <li><b>Automated Tuning:</b> Advanced techniques or machine-learning-based methods can systematically search for the optimal parameters.</li>
    <li><b>Simulation Before Real Deployment:</b> Use a simulator to test and tune gains in a safe environment.</li>
</ul>


        <!-- Additional Explanation on Actual Tuning Values -->
<h3>Additional Notes on Tuning Values</h3>
<p>
    In the code, we used:
</p>
<pre><code>self.Kp = 0.0013<br>
self.Kd = 0.005<br>
self.Ki = 0.001<br>
self.max_integral = 20  # For anti-windup<br>
</code></pre>
<p>
    These values were primarily found through trial and error. We started from a baseline (like <code>0.05</code>) and then:
</p>
<ul>
    <li><b>Reduced <code>Kp</code></b> until the oscillations were minimized but the robot still corrected its course in time.</li>
    <li><b>Adjusted <code>Kd</code></b> to dampen overshoot. Higher <code>Kd</code> provides quicker damping but can amplify noise.</li>
    <li><b>Introduced <code>Ki</code></b> in small increments to remove any steady-state error. 
        Once the robot could maintain a centered position, <code>Ki</code> was deemed sufficient.</li>
    <li><b>Set <code>max_integral</code></b> to <code>20</code> to cap the integral growth and prevent windup issues.</li>
</ul>
        


            <h3>A video of the Duckiebot going straight using P controller</h3>
            <video width="auto" height="600" controls>
                <source src="videos/P_S.mov" type="video/mp4">
                Your 
            </video>
            <p>
            The Duckiebot moves in a "snake-like" manner because the proportional (P) controller 
            is constantly correcting the steering based on the current error. Here, <code>Kp</code> is set to 
            <code>0.0013</code>. With this gain, the robot can still overshoot the center line repeatedly due to 
            inherent latency or noise, causing oscillatory (back-and-forth) motion. If <code>Kp</code> is too high, the 
            corrections become more aggressive, leading to increased swerving. Conversely, setting it too low 
            might result in sluggish steering and prolonged time to correct deviations.
        </p>
                 
            <h3>A video of the Duckiebot going straight using PI controller</h3>
            <video width="auto" height="600" controls>
                <source src="videos/PI_S.mov" type="video/mp4">
                Your 
            </video>
        <p>
        With the PD controller, the proportional term (<code>Kp</code>) corrects for the immediate lane deviation, 
        while the derivative term (<code>Kd</code> = <code>0.005</code>) helps dampen rapid changes to reduce 
        overshoot. Initially, the Duckiebot still exhibits a slight “snake-like” motion, but the derivative action 
        gradually brings it closer to the center line with fewer oscillations over time. However, there is still 
        some minor wiggle because the derivative term can amplify small measurement noises and minor latencies, 
        causing the controller to make frequent steering adjustments.
    </p>

            <h3>A video of the Duckiebot going straight using PID controller</h3>
            <video width="auto" height="600" controls>
                <source src="videos/PID_s.mov" type="video/mp4">
                Your 
            </video>
        <p>
        In the PID controller, the integral term (<code>Ki</code> = <code>0.001</code>) helps eliminate 
        the steady-state error so that the Duckiebot remains closer to the exact center of the lane 
        as it moves. The proportional term provides immediate corrections, while the derivative term 
        reduces overshoot and smooths out larger oscillations. As a result, the Duckiebot exhibits 
        more stable behavior compared to the P or PD controllers, maintaining a straighter path 
        with only minor deviations around the center.
    </p>


        <h1>Part 3: Lane Following</h1>
        <h2>1. Creating a Lane-Following Node</h2>
<p>
    The goal is to integrate computer vision (OpenCV) with a controller (P or PD) to 
    follow a lane for a full lap. We convert the camera feed to an OpenCV image, detect 
    the lane boundaries (e.g., using color thresholding or other techniques), and compute 
    an error signal representing how far the robot is from the center of the lane. 
    This error is then used by the controller to adjust the wheel speeds in order to keep 
    the robot aligned and moving forward.
</p>

<h2>2. Starting with a Basic Proportional Controller (P)</h2>
<p>
    In a proportional (P) controller, the control output is directly proportional to the 
    error. If <code>error</code> is the difference between the lane center and the robot’s 
    camera center, then:
</p>
<pre><code>control = Kp * error
</code></pre>
<p>
    <strong>Behavior in Practice:</strong>
</p>
<ul>
    <li><strong>Difficulty in Straight-Line Movement:</strong> 
        Sometimes the robot struggles to move straight. Even slight noise or minor camera offsets 
        can cause the robot to oscillate or veer to one side. If <code>Kp</code> is too large, the 
        robot aggressively corrects, causing overshoot. If <code>Kp</code> is too small, it is slow to 
        respond and may drift off course.
    </li>
    <li><strong>Handling Large Errors:</strong> 
        When encountering a large deviation (e.g., the robot starts far from the lane center or 
        goes around a sharp curve), the large error leads to a large steering correction. This 
        can cause abrupt turns and overshoot, making the robot swing widely from one side of the 
        lane to the other.
    </li>
    <li><strong>Oval-Shaped Track Issues:</strong> 
        On curves or oval segments, the path curvature can be quite large. The P controller 
        attempts to correct for sudden increases in error by turning sharply. Once it’s done 
        correcting, it may overcompensate and drift into another lane or continue oscillating, 
        especially if <code>Kp</code> is not carefully tuned. As a result, the robot might appear 
        to “go crazy” or oscillate dramatically around tight curves.
    </li>
</ul>

<h2>3. Transition to PD Controller</h2>
<p>
    By adding a derivative component (<code>Kd * derivative</code>), the controller can anticipate 
    changes in error and slow down corrections before the robot overshoots. The PD control law is:
</p>
<pre><code>control = Kp * error + Kd * (error - previous_error)
</code></pre>
<p>
    <strong>Behavior and Observations:</strong>
</p>
<ul>
    <li><strong>Smoother Response:</strong> 
        The derivative term helps reduce the overshoot that is common in purely proportional control. 
        The robot typically shows fewer oscillations because <code>Kd</code> dampens large, rapid changes 
        in error.
    </li>
    <li><strong>Handling Large Errors:</strong> 
        While the derivative term helps slow down corrections, very large errors (like starting off-center 
        or navigating tight corners on an oval track) can still cause the robot to veer quickly. The PD 
        controller will attempt to compensate, but if <code>Kd</code> is not tuned properly, you might 
        still see the robot zigzag between lane boundaries.
    </li>
    <li><strong>Oval-Track Behavior:</strong>
        On an oval or curved track, the robot might do better than with the P controller alone—less 
        oscillation and quicker stabilization. However, if the derivative gain is too high, noise from 
        camera readings or slight fluctuations can trigger overreactions. In some instances, the robot 
        may swing from left lane to right lane if it experiences a sudden large error, then corrects 
        again, continuing a “hunting” pattern across the track. 
    </li>
</ul>

<p>
    In summary, the PD controller generally results in smoother performance than P alone, 
    but it still requires careful tuning of <code>Kp</code> and <code>Kd</code> to avoid 
    excessive noise amplification or slow, sluggish responses around curves.
</p>


<h2>4. Transition to PID Controller</h2>

<h3> Following modification made to PID Controller for straight line task in Part 2 </h3>h3
<h4>Anti-Windup and Adaptive Speed in PID</h2>

<p>
    Below are two important code snippets from the PID controller that address 
    common issues in lane following: 
</p>

<ol>
    <li><strong>Integral Windup Prevention</strong></li>
    <li><strong>Adaptive Speed Control for Curves</strong></li>
</ol>

<h4>1. Anti-Windup Mechanism</h3>
<pre><code># Anti-windup: Clamp integral value to the range [0, max_integral]<br>
if self.integral >= self.max_integral:<br>
    self.integral -= error<br>
    if self.integral <= 75:<br>
        self.integral *= 0.80<br>
elif self.integral < 0:<br>
    self.integral = 0<br>
</code></pre>

<p>
    The integral term (<code>self.integral</code>) accumulates the error over time. If this error 
    persists for a long period (e.g., the robot is stuck or significantly off track), 
    the integral can become excessively large. This is known as “integral windup,” and 
    it can cause the controller to apply unnecessarily large corrections, leading to 
    oscillations or erratic motion.
</p>
<p>
    In this code, we clamp <code>self.integral</code> within the range 
    <code>[0, self.max_integral]</code>. If the integral grows beyond 
    <code>self.max_integral</code>, we subtract out the most recent error and optionally 
    scale the integral back (e.g., multiplying by <code>0.80</code>) to ensure it remains 
    within a safe range. If <code>self.integral</code> becomes negative, 
    it is reset to 0. This prevents the integral from ballooning out of control or 
    becoming negative beyond what is meaningful for the controller.
</p>

<h4>2. Adaptive Speed for Curves</h3>
<pre><code># Adjust base speed depending on the error (for better performance on curves)<br>
# When the error exceeds the curve_threshold, use a slower speed<br>
if abs(error) > self.curve_threshold:<br>
    base_speed = self.slow_speed<br>
else:<br>
    base_speed = self.normal_speed<br>
</code></pre>

<p>
    This snippet dynamically adjusts the robot’s forward speed based on the 
    magnitude of the lane-centering <code>error</code>. A large absolute error 
    (<code>abs(error)</code>) typically indicates a sharp curve or a significant deviation 
    from the lane center. In these cases, a slower speed (<code>self.slow_speed</code>) 
    helps the robot handle the turn more reliably and reduces the chance of overshoot.
</p>
<p>
    Conversely, when the error is small (the lane is relatively straight or the 
    robot is close to the center), the robot moves at a higher forward speed 
    (<code>self.normal_speed</code>). This balances efficiency on straight segments 
    with stability and caution on sharp curves.
</p>

<hr/>

<h3>Question: “Try adding the integral term to your PD controller and implement a PID control logic. Does this help with your control?”</h2>
<p>
    Yes, incorporating the integral term into the PD controller (thereby creating 
    a PID controller) can help eliminate steady-state error. Here’s why:
</p>
<ul>
    <li><strong>Elimination of Offset:</strong> The integral term accumulates small persistent errors that 
    the proportional term alone cannot fully correct, ensuring the robot stays aligned 
    with the lane center over time.</li>
    <li><strong>Potential Drawback - Windup:</strong> Without an anti-windup mechanism, the integral 
    can become too large if the robot remains far from the lane for an extended period. 
    This can lead to excessive corrective actions, causing oscillation. That’s why the 
    clamping and conditional adjustments of <code>self.integral</code> are important.</li>
    <li><strong>Better Handling of Imperfections:</strong> The integral term helps correct 
    subtle, continuous biases—like slight miscalibration in the camera or wheels—ensuring 
    a more stable center-line following.</li>
</ul>
<p>
    In practice, a properly tuned PID controller (with the integral gain set moderately 
    and combined with anti-windup) typically shows less steady-state offset and smoother 
    lane centering. However, you must balance <code>Kp</code>, <code>Ki</code>, and <code>Kd</code> 
    carefully to avoid introducing new oscillations or slow response.
</p>




        

            <h3>A video of the Duckiebot doing lane-following</h3>
            <video width="auto" height="600" controls>
                <source src="videos/our_lane_follow.mov" type="video/mp4">
                Your 
            </video>
        <p>
        In this demonstration, the Duckiebot is running a PID controller specifically tuned for 
        an oval-shaped track. The proportional term keeps the robot steering toward the lane center, 
        the integral term compensates for any lingering offset or bias, and the derivative term 
        prevents overshoot on curves. The result is a smooth path around the oval without stops 
        or external intervention. Even when encountering tighter turns, the Duckiebot maintains 
        stability and remains centered, thanks to adaptive speed adjustments and the integral 
        windup prevention mechanism.
    </p>





    </section>
                
    <section class="references">
        <h2>References</h2>
        <ul>
            <li>ChatGPT was used for help with making the website and understanding the topics like P, PD and PID controllers as well as to generate the report.</li>
            <li> 
                <a href="https://samuelfneumann.github.io/posts/duckie_4/">Duckietown Blog - Lane Detection</a>
            </li>
            
        </ul>
    </section>
    <h3>By: Sandhya Adhikari and Fumanpreet Singh</h1>
    
    <a href="index.html">Back to Assignments</a>
</body>
</html>
 
