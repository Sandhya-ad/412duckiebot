<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assignment 3</title>
    <link rel="stylesheet" href="templates/assignment3.css">
</head>
<body>
    <h1>Assignment 3 </h1>
    <section class="deliverables">
        <h1>Part 1: Computer Vision</h1>
        <ul>
            <li>
                A screen capture of the distorted camera image
                <div class="image-container">
                    <img src="images/distorted.png" alt="Distorted image" class="deliverable-photo">
                </div>
                <div class="report">
                    <h2>Camera Distortion</h2>
                    <p>
                        Camera distortion occurs due to the curvature of the lens, which bends light unevenly as it enters the camera sensor. This effect is particularly noticeable in radial distortion, where straight lines appear curved, especially near the edges of the image. We can see the same thing happen to our camera image in the above picture, especially near the edges, lines appear bent
                        </p>
                    <p>
                        <strong>Subscribing to camera topic: </strong>
                        To capture a distorted image, we subscribe to the ROS topic /camera_node/image/compressed, which provides a compressed stream of images from the camera. The subscription allows us to receive and process real-time camera data in ROS.
                    </p>
                </div>
            </li>

            <li>
                A screen capture of the undistorted camera image
                <div class="image-container">
                    <img src="images/undistorted_blur.png" alt="Distorted image" class="deliverable-photo">
                </div>
                <div class="report">
                    <h2>Undistorting the Image</h2>
                    <p>
                        To correct lens distortion, we use <strong> intrinsic parameters</strong> that describe the cameraâ€™s internal properties, such as focal length, optical center, and distortion coefficients. These parameters allow us to mathematically reverse the distortion effect and obtain a properly aligned image.
                    </p>
                    <p>
                        By performing these steps, we ensure that the camera feed provides an accurate and geometrically correct 
                        representation of the environment, which is essential for tasks such as color detection and lane following.
                    </p>
                    
                    <h3>What we did</h3>
                    
                    <ul>
                        <li>
                            <strong>Subscribing to the camera topic:</strong>
                            The robot receives real-time images from the ROS topic <code>/camera_node/image/compressed</code>. 
                            These images contain lens distortions that need correction before they can be used for accurate perception.
                        </li>
                        
                        <li>
                            <strong>Retrieving camera intrinsic parameters:</strong>
                            The robot's camera has unique calibration parameters stored in <code>/camera_node/camera_info</code>. 
                            These include:
                            <ul>
                                <li><strong>Camera Matrix:</strong> Defines how the camera maps the 3D world to a 2D image.</li>
                                <li><strong>Distortion Coefficients:</strong> Corrects radial and tangential distortions.</li>
                            </ul>
                            Using these parameters ensures that undistortion is tailored specifically for the robot's camera.
                        </li>
                        
                        <li>
                            <strong>Calculating an optimal new camera matrix:</strong>
                            Using <code>cv2.getOptimalNewCameraMatrix()</code>, we refine the camera matrix to minimize unwanted black borders 
                            in the corrected image. This step improves the visual quality of the output.
                        </li>
                    
                        <li>
                            <strong>Applying image correction:</strong>
                            The <code>cv2.undistort()</code> function is used to transform the distorted image into an undistorted one. 
                            This ensures that straight lines appear as they should, and objects are accurately represented.
                        </li>
                    
                        <li>
                            <strong>Publishing the corrected image:</strong>
                            The final undistorted image is then published to <code>/camera_undistorted/image/compressed</code>. 
                            This allows other ROS nodes, such as those responsible for <strong>color detection and lane following</strong>, to work 
                            with a geometrically correct image.
                        </li>
                    </ul>
                    
                    <h3>Purpose:</h3>

                    <p>
                        Undistortion is essential for accurate lane detection and navigation. When an image is distorted, straight lane markings can appear curved, leading to incorrect edge detection and misalignment. By correcting distortion, the robot can accurately perceive lane boundaries and navigate accordingly. Additionally, distorted images can stretch or compress objects, causing inconsistencies in object sizing. Undistortion ensures that objects appear with correct proportions, improving the accuracy of both lane detection and obstacle recognition. Color detection also benefits from undistortion, as color inconsistencies often occur near the edges of distorted images. By stabilizing the image, the robot can reliably detect lane colors without interference from warped color transitions. Finally, distance estimation depends on an accurate visual representation of the environment. Since distorted images alter the perceived shape and position of objects, correcting the distortion ensures that distance calculations remain precise, leading to more reliable navigation and decision-making.
                    </p>
                    
                    <ul>
                        <li>
                            <h2>Lane Detection Contours</h2>
                            <p>
                                The images below show the detected contours for <strong>blue, red, and green lanes</strong> using 
                                our ROS subscriber in <code>rqt_image_view</code>. The contours are drawn based on 
                                <strong>color segmentation and edge detection</strong>, ensuring accurate lane identification.
                            </p>
            
                            <h3>Contours Detecting a Blue Line</h3>
                            <div class="image-container">
                                <img src="images/blue_detection.png" alt="Blue Lane Contours" class="deliverable-photo">
                            </div>
                            <p>
                                The above image shows how the <strong>blue lane</strong> is detected. The contours are drawn 
                                around the largest detected blue area using <strong>HSV filtering and contour extraction</strong>.
                            </p>
            
                            <h3>Contours Detecting a Red Line</h3>
                            <div class="image-container">
                                <img src="images/red_detection.png" alt="Red Lane Contours" class="deliverable-photo">
                            </div>
                            <p>
                                This image demonstrates the detection of the <strong>red lane</strong>. Since red appears in two 
                                sections of the <strong>HSV color space</strong>, we use two separate hue ranges to ensure complete 
                                detection. The detected lane is highlighted with contours.
                            </p>
            
                            <h3>Contours Detecting a Green Line</h3>
                            <div class="image-container">
                                <img src="images/green_detection.png" alt="Green Lane Contours" class="deliverable-photo">
                            </div>
                            <p>
                                The <strong>green lane</strong> is detected by applying a mask within the green HSV range. The 
                                contours drawn on the detected area highlight the extracted lane.
                            </p>
                        </li>
            
                        <li>
                            <h2>Explanation of Color Detection</h2>

                            <h3>Preprocessing the Image</h3>
                            <p>
                                The camera feed is first undistorted to correct for lens distortion. 
                                Then, the image is converted from </strong>BGR to HSV format</strong> to separate 
                                color components, making color segmentation more reliable.
                            </p>
                            <p>
                                Color detection is performed by converting the <strong>BGR image to HSV format</strong> and 
                                applying predefined HSV thresholds. The <strong>HSV (Hue, Saturation, Value) color model</strong> 
                                is preferred because it separates color intensity from brightness, making it more 
                                reliable under different lighting conditions.
                            </p>
            
                            <h2>How Color Detection Works</h2>
                            <h3> Applying Color Thresholding</h3>
                            <p>
                                The HSV model is used to filter out specific colors corresponding to 
                                <strong>red, blue, and green lanes</strong>. The following ranges are applied:
                            </p>
                            <ul>
                                <li><strong>Red Lane:</strong> <code>Lower: [0, 100, 100] & [170, 100, 100], Upper: [10, 255, 255] & [180, 255, 255]</code></li>
                                <li><strong>Blue Lane:</strong> <code>Lower: [100, 150, 100], Upper: [130, 255, 255]</code></li>
                                <li><strong>Green Lane:</strong> <code>Lower: [46, 50, 65], Upper: [95, 196, 199]</code></li>
                            </ul>
                            <p>
                                Each color mask isolates only the pixels matching the specified HSV range, 
                                filtering out everything else.
                            </p>
            
                            <h3>3. Removing Noise</h3>
                            <p>
                                Morphological operations, such as </strong>closing</strong>, are applied to remove 
                                small artifacts that could cause false detections.
                            </p>
            
                            <h3>4. Finding and Drawing Contours</h3>
                            <p>
                                Using <code>cv2.findContours()</code>, lane boundaries are detected 
                                and enclosed in a bounding box using <code>cv2.boundingRect()</code>. 
                                The largest detected contour for each color is selected.
                            </p>
            
                            <h3>5. Estimating Distance to the Lane</h3>
                            <p>
                                The width of the detected lane in pixels is used to estimate its 
                                real-world distance using the <strong>pinhole camera model</strong>:
                            </p>
                            <pre>
    distance_m = (known_object_width_m * focal_length_px) / object_pixel_width
                            </pre>
                            <p>
                                The known width of the lane is a predefined constant, and the 
                                focal length is retrieved from the cameraâ€™s intrinsic parameters.
                                This is used to see how far the object is from the robot
                            </p>
            
                            <h3>6. Publishing the Detected Lane</h3>
                            <p>
                                The processed image, with detected lanes and distance labels, 
                                is published to <code>/lane_detection/image/compressed</code> for 
                                integration with navigation modules.
                                The dimensions of the detected lane is also published in the terminal.
                                Example of our output:
                                <pre>
    [INFO] [1740622024.705454]: Published lane detection image.
    [INFO] [1740622024.751194]: Detected: red, Dimensions: (240, 23), Distance: 0.223m
    [INFO] [1740622024.772927]: Published lane detection image.
    [INFO] [1740622024.830412]: Detected: red, Dimensions: (242, 22), Distance: 0.221m
                                </pre>

                            </p>
                        </li>
            
                        <li>
                            <h2>Summary of Detection for Each Color</h2>
            
                            <h3>Red Lane Detection</h3>
                            <p>
                                - <strong>Uses two HSV ranges</strong> to capture different red shades.
                                - Filters out yellow regions that may overlap with red.
                                - Extracts the <strong>largest contour</strong> and highlights it.
                                - <strong>Used to detect stop points</strong> and straight movement areas.
                            </p>
            
                            <h3>Blue Lane Detection</h3>
                            <p>
                                - Detects blue lanes using a <strong>tight HSV range</strong>.
                                - Filters out shadows and dark patches.
                                - Contours are enclosed in a bounding box.
                                - <strong>Used to trigger right turns</strong> in the navigation system.
                            </p>
            
                            <h3>Green Lane Detection</h3>
                            <p>
                                - Filters out unwanted environmental colors.
                                - Detects <strong>left-turn trigger points</strong>.
                                - Bounding boxes are drawn around the detected lane.
                            </p>
                        </li>
                    </ul>
                        </li>
            
                        <li>
                            <h2>How to Tune HSV Parameters</h2>
                            <p>
                                The HSV values for each lane color are adjusted to optimize detection. The tuning process involves:
                            </p>
                            <ul>
                                <li>
                                    <strong>Increasing or decreasing the Hue (H) range</strong> to fine-tune color recognition. 
                                    If a color is not detected properly, the hue range is <strong>expanded or shifted</strong>.
                                </li>
                                <li>
                                    <strong>Adjusting the Saturation (S) range</strong> to control how vivid the colors need to be. 
                                    This helps filter out <strong>faded colors</strong> and background noise.
                                </li>
                                <li>
                                    <strong>Modifying the Value (V) range</strong> to adjust brightness sensitivity. Raising the 
                                    lower limit reduces <strong>false detections in dark regions</strong>.
                                </li>
                                <li>
                                    <strong>Testing under different lighting conditions</strong> to ensure the values work consistently.
                                </li>
                            </ul>
                            <p>
                                The final <strong>HSV thresholds</strong> used for lane detection are:
                            </p>
                            <ul>
                                <li><strong>Blue Lane:</strong> <code>Lower: [100, 150, 100], Upper: [130, 255, 255]</code></li>
                                <li><strong>Red Lane:</strong> <code>Lower: [0, 100, 100] & [170, 100, 100], Upper: [10, 255, 255] & [180, 255, 255]</code></li>
                                <li><strong>Green Lane:</strong> <code>Lower: [46, 50, 65], Upper: [95, 196, 199]</code></li>
                            </ul>
                        </li>
                    </ul>
                    <ul>
                        <li>
                            <h3>A video of the Duckiebot going straight until red line stopping and moving 30 cm ahead</h3>
                            <video width="auto" height="600" controls>
                                <source src="videos/red_lane.mp4" type="video/mp4">
                                Your 
                            </video>
                        </li>
                        <li>
                            <h3>A video of the Duckiebot going straight until blue line stopping then turning right after turning the right lights to red</h3>
                            <video width="auto" height="600" controls>
                                <source src="videos/blue_lane.mp4" type="video/mp4">
                                Your 
                            </video>
                        </li>
                        <li>
                            <h3>A video of the Duckiebot going straight until green line stopping then turning left after turning the left lights to red</h3>
                            <video width="auto" height="600" controls>
                                <source src="videos/blue_lane.mp4" type="video/mp4">
                                Your 
                            </video>
                        </li>
                    </ul>
                    <h1>Lane Behavior Execution Report</h1>
                    <section class="deliverables">
                        
                        <h2>Detecting Lane Colors</h2>
                        <p>
                            The Duckiebot detects lanes using a camera feed, which is processed in real time. The image is converted from BGR to HSV format, allowing the system to separate color information efficiently. Specific HSV thresholds are applied to filter out the target colors:
                        </p>
                        <ul>
                            <li><strong>Red Lane:</strong> Uses two separate HSV ranges to capture different red shades.</li>
                            <li><strong>Blue Lane:</strong> Detected using a tight HSV range to avoid false positives.</li>
                            <li><strong>Green Lane:</strong> A mask is applied within the green HSV range to isolate the green lane.</li>
                        </ul>
                        
                        <h2>Estimating Distance to the Lane</h2>
                        <p>
                            The detected lane width in pixels is used to estimate the real-world distance. The pinhole camera model formula is applied:
                        </p>
                        <pre>
                    distance_m = (known_lane_width_m * focal_length_px) / detected_pixel_width
                        </pre>
                        <p>
                            This ensures accurate movement decisions based on how far the lane is from the robot.
                        </p>
                        
                        <h2>LED Signaling</h2>
                        <p>
                            The robot uses LEDs to signal turning intentions:
                        </p>
                        <ul>
                            <li>For a <strong>right turn (blue lane)</strong>, the right-side LEDs (front & back) turn red.</li>
                            <li>For a <strong>left turn (green lane)</strong>, the left-side LEDs (front & back) turn red.</li>
                            <li>For a <strong>red lane stop</strong>, no LED signaling is needed.</li>
                        </ul>
                        <p>The LED colors are controlled through ROS by publishing a pattern to the LED topic.</p>
                        
                        <p>
                            The Duckiebot uses LED indicators to signal turning behavior based on detected lanes. The LED system consists of 
                            five individual lights positioned around the robot, each assigned a specific index:
                        </p>
                        <ul>
                            <li><strong>Index 0:</strong> Front-left LED</li>
                            <li><strong>Index 1:</strong> Back-right LED</li>
                            <li><strong>Index 2:</strong> Center LED (not used for signaling)</li>
                            <li><strong>Index 3:</strong> Back-left LED</li>
                            <li><strong>Index 4:</strong> Front-right LED</li>
                        </ul>

                        <h3>Right Turn Signal (Blue Lane)</h3>
                        <p>
                            When the robot detects a blue lane, it activates the <strong>front-right (index 4) and back-right (index 1) LEDs</strong> to signal a right turn. 
                            This is achieved by publishing a pattern where only these indices are set to active while the others remain off.
                        </p>
                        <p>
                            The LED activation pattern for a right turn is:
                        </p>
                        <pre>
                        [0,1,0,0,1]  <!-- Only back-right and front-right LEDs are ON -->
                        </pre>
                        <p>
                            This pattern is sent to the Duckiebot's LED topic:
                        </p>
                        <pre>
                        /csc22907/led_emitter_node/led_pattern
                        </pre>

                        <h3>Left Turn Signal (Green Lane)</h3>
                        <p>
                            When the robot detects a green lane, it activates the <strong>front-left (index 0) and back-left (index 3) LEDs</strong> to signal a left turn. 
                            The activation pattern ensures only the required LEDs are turned on.
                        </p>

                        
                        <h2>Executing Movement</h2>
                        <p>
                            The Duckiebot executes specific behaviors based on the detected lane color:
                        </p>
                        <ul>
                            <li><strong>Blue Lane:</strong> The robot stops, signals a right turn, then performs a 90-degree right curve.</li>
                            <p>
                                When the duckiebot detects blue lane, it calculates how much distance to move and that distance is sent to a straight moving function to move upto that distance. It stops after moving for that distance and does the right turn.
                            </p>
                            <li><strong>Red Lane:</strong> After detecting the red, calculates the distance, move upto the point, then robot stops for 3 seconds and moves forward by 30 cm.</li>

                            <li><strong>Green Lane:</strong> Moves upto the point, stops, signals a left turn, then executes a 90-degree left curve.</li>
                        </ul>
                        <p>
                            The movement is executed using wheel encoder data to ensure accurate positioning.
                        </p>
                        
                        <h2>Execution Log</h2>
                        <pre>
                    [INFO] Lane detected: blue
                    [INFO] Stopping for 3 seconds.
                    [INFO] Signaling right turn (right LEDs red).
                    [INFO] Executing 90-degree right curve.
                    [INFO] Lane detected: red
                    [INFO] Stopping for 3 seconds.
                    [INFO] Moving forward 30 cm.
                    [INFO] Lane detected: green
                    [INFO] Stopping for 3 seconds.
                    [INFO] Signaling left turn (left LEDs red).
                    [INFO] Executing 90-degree left curve.
                        </pre>
                    </section>
                </div>
            </li>
        <h1>Part 2: Controllers</h1>
        <p>
            This section explores in depth different types of controllers that enable the bot to drive
            autonomously.
        </p>


    </section>
                
    <section class="references">
        <h2>References</h2>
        <ul>
            <li>ChatGPT was used for help with making the website and understanding some topics like P, PD and PID controllers</li>
            <li> 
                <a href="https://samuelfneumann.github.io/posts/duckie_4/">Duckietown Blog - Lane Detection</a>
            </li>
            
        </ul>
    </section>
    <h3>By: Sandhya Adhikari and Fumanpreet Singh</h1>
    
    <a href="index.html">Back to Assignments</a>
</body>
</html>
 
